{"Id": "45109533", "PostTypeId": "1", "CreationDate": "2017-07-14T18:28:01.780", "Score": "63", "ViewCount": "73669", "Body": "<p>I'm trying to copy files locally from s3 bucket. I can get the list of files on my bucket:</p>\n\n<pre><code>aws s3 ls  s3://myBucket/myDirectory/todaysFiles/\n</code></pre>\n\n<p>But when I try to copy the files locally:</p>\n\n<pre><code>aws s3 cp s3://myBucket/myDirectory/todaysFiles/ .\n</code></pre>\n\n<p>I get this error:</p>\n\n<pre><code>fatal error: An error occurred (404) when calling the HeadObject operation: Key \"myDirectory/todaysFiles/\" does not exist\n</code></pre>\n\n<p>But I try to copy just one file locally:</p>\n\n<pre><code> aws s3 cp s3://myBucket/myDirectory/todaysFiles/somefile .\n</code></pre>\n\n<p>I get this error:</p>\n\n<pre><code> warning: Skipping file s3://myBucket/myDirectory/todaysFiles/somefile. Object is of storage class GLACIER. Unable to perform download operations on GLACIER objects. You must restore the object to be able to the perform operation. See aws s3 download help for additional parameter options to ignore or force these transfers.\n</code></pre>\n\n<p>Any of you knows why I'm getting this error or way around this errors?</p>\n\n<p>I really appreciate your help</p>\n", "OwnerUserId": "2924482", "LastActivityDate": "2021-05-30T07:58:21.660", "Title": "AWS CLI S3: copying file locally using the terminal : fatal error: An error occurred (404) when calling the HeadObject operation", "Tags": "|amazon-web-services|amazon-s3|aws-sdk|aws-cli|aws-cloudformation|", "AnswerCount": "4", "CommentCount": "0", "FavoriteCount": "0", "ContentLicense": "CC BY-SA 3.0", "history": [{"Id": "151614452", "PostHistoryTypeId": "2", "PostId": "45109533", "RevisionGUID": "d21f6a3b-53d8-4288-88aa-2cbdfc8bfda1", "CreationDate": "2017-07-14T18:28:01.780", "UserId": "2924482", "Text": "I'm trying to copy files locally from s3 bucket. I can get the list of files on my bucket:\r\n\r\n    aws s3 ls  s3://myBucket/myDirectory/todaysFiles/\r\n\r\nBut when I try to copy the files locally:\r\n\r\n    aws s3 cp s3://myBucket/myDirectory/todaysFiles/ .\r\n\r\nI get this error:\r\n\r\n    fatal error: An error occurred (404) when calling the HeadObject operation: Key \"myDirectory/todaysFiles/\" does not exist\r\n\r\nBut I try to copy just one file locally:\r\n\r\n     aws s3 cp s3://myBucket/myDirectory/todaysFiles/somefile .\r\n\r\nI get this error:\r\n\r\n     warning: Skipping file s3://myBucket/myDirectory/todaysFiles/somefile. Object is of storage class GLACIER. Unable to perform download operations on GLACIER objects. You must restore the object to be able to the perform operation. See aws s3 download help for additional parameter options to ignore or force these transfers.\r\n\r\nAny of you knows why I'm getting this error or way around this errors?\r\n\r\nI really appreciate your help\r\n\r\n\r\n", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "Object is of storage class GLACIER. ", "keywords": ["storage"]}]}, {"Id": "151614453", "PostHistoryTypeId": "1", "PostId": "45109533", "RevisionGUID": "d21f6a3b-53d8-4288-88aa-2cbdfc8bfda1", "CreationDate": "2017-07-14T18:28:01.780", "UserId": "2924482", "Text": "AWS CLI S3: copying file locally using the terminal : fatal error: An error occurred (404) when calling the HeadObject operation", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}, {"Id": "151614454", "PostHistoryTypeId": "3", "PostId": "45109533", "RevisionGUID": "d21f6a3b-53d8-4288-88aa-2cbdfc8bfda1", "CreationDate": "2017-07-14T18:28:01.780", "UserId": "2924482", "Text": "|amazon-web-services|amazon-s3|aws-sdk|aws-cli|aws-cloudformation|", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}], "answers": [{"Id": "67758996", "PostTypeId": "2", "ParentId": "45109533", "CreationDate": "2021-05-30T07:58:21.660", "Score": "16", "Body": "<p>Use --recursive at the end if the Source contains multiple folders.</p>\n<pre><code>aws s3 cp s3Uri Local --recursive\n</code></pre>\n", "OwnerUserId": "9041758", "LastActivityDate": "2021-05-30T07:58:21.660", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "247550121", "PostHistoryTypeId": "2", "PostId": "67758996", "RevisionGUID": "5b5adf5f-9bfe-4fa5-86eb-028e7f23c997", "CreationDate": "2021-05-30T07:58:21.660", "UserId": "9041758", "Text": "Use --recursive at the end if the Source contains multiple folders.\r\n\r\n    \r\n\r\n    aws s3 cp s3Uri Local --recursive", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "filtered-sentences": []}, {"Id": "67336568", "PostTypeId": "2", "ParentId": "45109533", "CreationDate": "2021-04-30T15:28:51.143", "Score": "0", "Body": "<p>In case it helps anyone, you can also use</p>\n<pre><code>aws s3 sync s3://&lt;bucketname&gt;/&lt;folder&gt;/ ./&lt;folder&gt;\n</code></pre>\n<p>This should work without having to use the recursive. It has the benefit that it skips files already present locally.</p>\n<p>As mentioned by @raju, glacier is a cheap storage system on AWS, meaning if you want to retrieve the data you first need to put a request in to fetch the data before you can download it. It's made to be cheap for long term storage that isn't accessed frequently.</p>\n", "OwnerUserId": "252685", "LastActivityDate": "2021-04-30T15:28:51.143", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "245718902", "PostHistoryTypeId": "2", "PostId": "67336568", "RevisionGUID": "09a26abc-0409-4171-8ed5-8f556809b5f3", "CreationDate": "2021-04-30T15:28:51.143", "UserId": "252685", "Text": "In case it helps anyone, you can also use \r\n\r\n    aws s3 sync s3://<bucketname>/<folder>/ ./<folder>\r\n\r\nThis should work without having to use the recursive. It has the benefit that it skips files already present locally.\r\n\r\nAs mentioned by @raju, glacier is a cheap storage system on AWS, meaning if you want to retrieve the data you first need to put a request in to fetch the data before you can download it. It's made to be cheap for long term storage that isn't accessed frequently.", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "As mentioned by @raju, glacier is a cheap storage system on AWS, meaning if you want to retrieve the data you first need to put a request in to fetch the data before you can download it. ", "keywords": ["cheap", "storage"]}, {"source": "Text", "text": "It's made to be cheap for long term storage that isn't accessed frequently.", "keywords": ["cheap", "storage"]}]}], "filtered-sentences": [{"source": "Body", "text": "As mentioned by @raju, glacier is a cheap storage system on AWS, meaning if you want to retrieve the data you first need to put a request in to fetch the data before you can download it. ", "keywords": ["cheap", "storage"]}, {"source": "Body", "text": "It's made to be cheap for long term storage that isn't accessed frequently.", "keywords": ["cheap", "storage"]}]}, {"Id": "49543137", "PostTypeId": "2", "ParentId": "45109533", "CreationDate": "2018-03-28T19:57:59.547", "Score": "-3", "Body": "<pre><code>pip install --upgrade --user awscli\n\naws s3 cp s3://inbound/test/ /inbound/ --recursive --region us-west-2\n</code></pre>\n", "OwnerUserId": "7654870", "LastEditorUserId": "5781745", "LastEditDate": "2018-03-28T22:24:02.153", "LastActivityDate": "2018-03-28T22:24:02.153", "CommentCount": "3", "ContentLicense": "CC BY-SA 3.0", "comments": [{"Id": "100530725", "PostId": "49543137", "Score": "0", "Text": "The `--region` switch applies to the destination bucket, which is meaningless here because it's the local file system. To specify the region of the source bucket you use the `--source-region` switch instead. REF: https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html", "CreationDate": "2019-07-11T23:43:44.013", "UserId": "390122", "filtered-sentences": []}, {"Id": "114830057", "PostId": "49543137", "Score": "0", "Text": "Looks like OP already had awscli installed, no need to suggest it.", "CreationDate": "2020-11-22T05:16:42.267", "UserId": "7348119", "filtered-sentences": []}, {"Id": "117723004", "PostId": "49543137", "Score": "0", "Text": "s3 is global isn't it? what's the point of specifying the region?", "CreationDate": "2021-03-12T05:08:01.097", "UserId": "4590025", "filtered-sentences": []}], "history": [{"Id": "170056878", "PostHistoryTypeId": "5", "PostId": "49543137", "RevisionGUID": "769c0527-b721-4999-80e3-ba1974dd2b12", "CreationDate": "2018-03-28T22:24:02.153", "UserId": "5781745", "Comment": "added 12 characters in body", "Text": "    pip install --upgrade --user awscli\r\n    \r\n    aws s3 cp s3://inbound/test/ /inbound/ --recursive --region us-west-2", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "pip install --upgrade --user awscli aws s3 cp s3://inbound/test/ /inbound/ --recursive --region us-west-2", "keywords": ["test"]}]}, {"Id": "170048718", "PostHistoryTypeId": "2", "PostId": "49543137", "RevisionGUID": "66a09b96-28dc-44b5-8421-e1d1d861756c", "CreationDate": "2018-03-28T19:57:59.547", "UserId": "7654870", "Text": "pip install --upgrade --user awscli\r\n\r\naws s3 cp s3://inbound/test/ /inbound/ --recursive --region us-west-2", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "pip install --upgrade --user awscli aws s3 cp s3://inbound/test/ /inbound/ --recursive --region us-west-2", "keywords": ["test"]}]}], "filtered-sentences": []}, {"Id": "45110302", "PostTypeId": "2", "ParentId": "45109533", "CreationDate": "2017-07-14T19:25:42.100", "Score": "107", "Body": "<p>For the first error - add the recursive flag:</p>\n\n<pre><code>aws s3 cp s3://myBucket/myDirectory/todaysFiles/ . --recursive\n</code></pre>\n\n<p>This will copy all the files in the \"todaysFiles\" directory to the current directory.</p>\n\n<p>However, the second error indicates that your files are in Glacier.  This complicates things a bit as Glacier is not real time - depending on what you're willing to pay it can be hours before the data is restored.  See the <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html\" rel=\"noreferrer\">Restoring Objects</a> docs for a bit more information.  You can't copy from S3 until the object are restored from Glacier to S3.</p>\n\n<p>Note that if you do this you will have costs from both Glacier and S3.</p>\n\n<p>As an aside, if these files are really files from today there should be a much longer time between storage on S3 and the push to Glacier.  But I'm guessing that the parent directories may have a date related component too.</p>\n", "OwnerUserId": "2933977", "LastActivityDate": "2017-07-14T19:25:42.100", "CommentCount": "1", "ContentLicense": "CC BY-SA 3.0", "comments": [{"Id": "130714992", "PostId": "45110302", "Score": "0", "Text": "--recursive, that fixed it for me", "CreationDate": "2022-10-11T16:54:16.657", "UserId": "321866", "filtered-sentences": []}], "history": [{"Id": "151617527", "PostHistoryTypeId": "2", "PostId": "45110302", "RevisionGUID": "f1ca6398-e496-46d9-ae10-28c54970b5dc", "CreationDate": "2017-07-14T19:25:42.100", "UserId": "2933977", "Text": "For the first error - add the recursive flag:\r\n\r\n    aws s3 cp s3://myBucket/myDirectory/todaysFiles/ . --recursive\r\n\r\nThis will copy all the files in the \"todaysFiles\" directory to the current directory.\r\n\r\nHowever, the second error indicates that your files are in Glacier.  This complicates things a bit as Glacier is not real time - depending on what you're willing to pay it can be hours before the data is restored.  See the <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html\">Restoring Objects</a> docs for a bit more information.  You can't copy from S3 until the object are restored from Glacier to S3.\r\n\r\nNote that if you do this you will have costs from both Glacier and S3.\r\n\r\nAs an aside, if these files are really files from today there should be a much longer time between storage on S3 and the push to Glacier.  But I'm guessing that the parent directories may have a date related component too.\r\n\r\n\r\n\r\n\r\n", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "This complicates things a bit as Glacier is not real time - depending on what you're willing to pay it can be hours before the data is restored. ", "keywords": ["pay"]}, {"source": "Text", "text": "As an aside, if these files are really files from today there should be a much longer time between storage on S3 and the push to Glacier. ", "keywords": ["storage"]}]}], "filtered-sentences": [{"source": "Body", "text": "This complicates things a bit as Glacier is not real time - depending on what you're willing to pay it can be hours before the data is restored. ", "keywords": ["pay"]}, {"source": "Body", "text": "As an aside, if these files are really files from today there should be a much longer time between storage on S3 and the push to Glacier. ", "keywords": ["storage"]}]}], "contains-topic": true, "filtered-sentences": []}
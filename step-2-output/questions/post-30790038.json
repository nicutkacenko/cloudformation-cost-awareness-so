{"Id": "30790038", "PostTypeId": "1", "AcceptedAnswerId": "34682862", "CreationDate": "2015-06-11T19:44:48.747", "Score": "20", "ViewCount": "12384", "Body": "<p>I've been investigating creating my own mongodb cluster in AWS. <a href=\"https://aws.amazon.com/blogs/aws/mongodb-on-the-aws-cloud-new-quick-start-reference-deployment/\">Aws mongodb template</a> provides some good starting points. However, it doesn't cover auto scaling or when a node goes down. For example, if I have 1 primary and 2 secondary nodes. And the primary goes down and auto scaling kicks in. How would I add the newly launched mongodb instance to the replica set? </p>\n\n<p>If you look at the template, it uses an init.sh script to check if the node being launched is a primary node and waits for all other nodes to exist and creates a replica set with thier ip addresses on the primary. When the Replica set is configured initailly, all the nodes already exist.</p>\n\n<p>Not only that, but my node app uses mongoose. Part of the database connection allows you to specify multiple nodes. How would I keep track of what's currently up and running (I guess I could use DynamoDB but not sure).</p>\n\n<p>What's the usual flow if an instance goes down? Do people generally manually re-configure clusters if this happens? </p>\n\n<p>Any thoughts? Thanks.</p>\n", "OwnerUserId": "865024", "LastActivityDate": "2024-01-24T10:55:12.633", "Title": "Mongodb cluster with aws cloud formation and auto scaling", "Tags": "|mongodb|amazon-web-services|cluster-computing|autoscaling|aws-cloudformation|", "AnswerCount": "3", "CommentCount": "0", "FavoriteCount": "0", "ContentLicense": "CC BY-SA 3.0", "history": [{"Id": "92632509", "PostHistoryTypeId": "2", "PostId": "30790038", "RevisionGUID": "497fe3ee-07bd-4222-b047-1b820850a66b", "CreationDate": "2015-06-11T19:44:48.747", "UserId": "865024", "Text": "I've been investigating creating my own mongodb cluster in AWS. [Aws mongodb template][1] provides some good starting points. However, it doesn't cover auto scaling or when a node goes down. For example, if I have 1 primary and 2 secondary nodes. And the primary goes down and auto scaling kicks in. How would I add the newly launched mongodb instance to the replica set? \r\n\r\nIf you look at the template, it uses an init.sh script to check if the node being launched is a primary node and waits for all other nodes to exist and creates a replica set with thier ip addresses on the primary. When the Replica set is configured initailly, all the nodes already exist.\r\n\r\nNot only that, but my node app uses mongoose. Part of the database connection allows you to specify multiple nodes. How would I keep track of what's currently up and running (I guess I could use DynamoDB but not sure).\r\n\r\nWhat's the usual flow if an instance goes down? Do people generally manually re-configure clusters if this happens? \r\n\r\nAny thoughts? Thanks.\r\n\r\n\r\n  [1]: https://aws.amazon.com/blogs/aws/mongodb-on-the-aws-cloud-new-quick-start-reference-deployment/", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I've been investigating creating my own mongodb cluster in AWS. ", "keywords": ["cluster"]}, {"source": "Text", "text": "How would I add the newly launched mongodb instance to the replica set? ", "keywords": ["instance"]}, {"source": "Text", "text": "What's the usual flow if an instance goes down? ", "keywords": ["instance"]}]}, {"Id": "92632510", "PostHistoryTypeId": "1", "PostId": "30790038", "RevisionGUID": "497fe3ee-07bd-4222-b047-1b820850a66b", "CreationDate": "2015-06-11T19:44:48.747", "UserId": "865024", "Text": "Mongodb cluster with aws cloud formation and auto scaling", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "Mongodb cluster with aws cloud formation and auto scaling", "keywords": ["cluster"]}]}, {"Id": "92632511", "PostHistoryTypeId": "3", "PostId": "30790038", "RevisionGUID": "497fe3ee-07bd-4222-b047-1b820850a66b", "CreationDate": "2015-06-11T19:44:48.747", "UserId": "865024", "Text": "|mongodb|amazon-web-services|cluster-computing|autoscaling|aws-cloudformation|", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "|mongodb|amazon-web-services|cluster-computing|autoscaling|aws-cloudformation|", "keywords": ["cluster"]}]}], "answers": [{"Id": "77548465", "PostTypeId": "2", "ParentId": "30790038", "CreationDate": "2023-11-25T15:33:39.820", "Score": "0", "Body": "<p>I have an other solution with ECS and EC2 autoscale group. This deployment can run with Spot Instance and autoscale.</p>\n<p>This is cloudformation template:\n<a href=\"https://github.com/congthang1/mongodb-aws-autoscale\" rel=\"nofollow noreferrer\">https://github.com/congthang1/mongodb-aws-autoscale</a></p>\n<p>Hope this help.</p>\n", "OwnerUserId": "4417228", "LastActivityDate": "2023-11-25T15:33:39.820", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "300573912", "PostHistoryTypeId": "2", "PostId": "77548465", "RevisionGUID": "c118f673-49df-4331-8bae-19bad4a44b5c", "CreationDate": "2023-11-25T15:33:39.820", "UserId": "4417228", "Text": "I have an other solution with ECS and EC2 autoscale group. This deployment can run with Spot Instance and autoscale.\r\n\r\nThis is cloudformation template:\r\n[https://github.com/congthang1/mongodb-aws-autoscale][1]\r\n\r\nHope this help.\r\n\r\n\r\n  [1]: https://github.com/congthang1/mongodb-aws-autoscale", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "This deployment can run with Spot Instance and autoscale. ", "keywords": ["instance"]}]}], "filtered-sentences": [{"source": "Body", "text": "This deployment can run with Spot Instance and autoscale. ", "keywords": ["instance"]}]}, {"Id": "77872429", "PostTypeId": "2", "ParentId": "30790038", "CreationDate": "2024-01-24T10:55:12.633", "Score": "0", "Body": "<h2>Primary Solution</h2>\n<p>I have been through this situation as well and then I finally decided to develop my own Terraform module to quickly deploy a secure, persistent, highly available, self healing, efficient, cost effective and self managed single-node or multi-node MongoDB NoSQL document database cluster on AWS ECS cluster with monitoring and alerting enabled.</p>\n<p>Terraform module can be found here: <a href=\"https://github.com/abdullahkhawer/mongodb-cluster-on-aws-ecs\" rel=\"nofollow noreferrer\">MongoDB Cluster on AWS ECS - Terraform Module</a></p>\n<h4>Original questions:</h4>\n<blockquote>\n<p>I've been investigating creating my own mongodb cluster in AWS. Aws mongodb template provides some good starting points. However, it doesn't cover auto scaling or when a node goes down. For example, if I have 1 primary and 2 secondary nodes. And the primary goes down and auto scaling kicks in. How would I add the newly launched mongodb instance to the replica set?</p>\n</blockquote>\n<blockquote>\n<p>If you look at the template, it uses an init.sh script to check if the node being launched is a primary node and waits for all other nodes to exist and creates a replica set with thier ip addresses on the primary. When the Replica set is configured initailly, all the nodes already exist.</p>\n</blockquote>\n<h4>Answer:</h4>\n<p>The solution that I developed is based on AWS ECS and AWS EC2 Autoscaling. So, whenever a task or container fails, a new task is created automatically and whenever an AWS EC2 instance (node) goes down, it creates a new. It is developed in a way that the node can rejoin the cluster again. And yes, you can create up to 3 nodes in it. So, yes, it is highly available and self healing as well. You just need create a replica set after logging into one of the nodes using AWS SSM session manager and that's it. If the node goes down or the container fails, it will automatically fill up the replica set again. Even if you lose the whole cluster, you can reconfigure. I have shared some instructions in the code on that part but that is highly unlikely to happen.</p>\n<h4>Another original question:</h4>\n<blockquote>\n<p>Not only that, but my node app uses mongoose. Part of the database connection allows you to specify multiple nodes. How would I keep track of what's currently up and running (I guess I could use DynamoDB but not sure).</p>\n</blockquote>\n<h4>Answer:</h4>\n<p>With my solution, you can keep track using AWS ECS or AWS CloudWatch to make sure everything is up and running fine. I have set AWS CloudWatch alarms as well to send alerts when the utilization of CPU, Memory and Disk Space goes beyond 85%. There is a a parameter created on AWS SSM Parameter Store that has all the nodes URLs so you can use them to connect to the cluster. For example, in my case, I have used <code>mongosh</code> to connect as follows: <code>mongosh &quot;mongodb://[USERNAME]:[PASSWORD]@[ENVIRONMENT_NAME]-mongodb1.[AWS_ROUTE_53_PRIVATE_HOSTED_ZONE_NAME]:27017,[ENVIRONMENT_NAME]-mongodb2.[AWS_ROUTE_53_PRIVATE_HOSTED_ZONE_NAME]:27017,[ENVIRONMENT_NAME]-mongodb3.[AWS_ROUTE_53_PRIVATE_HOSTED_ZONE_NAME]:27017/admin?replicaSet=rs0&amp;readPreference=secondaryPreferred&amp;retryWrites=true&quot;</code>\nYou can use mongoose as well.</p>\n<p>There are many key highlights of my solution but I'm not sharing here to keep the answer precise and concise.</p>\n<p>My solution is self managed and open source which gives us the ability to customize it as desired.</p>\n<h2>Secondary Solution</h2>\n<p>There is this partner solution available as well by MongoDB Atlas on the AWS Marketplace which can be tried. Link is here: <a href=\"https://aws.amazon.com/solutions/partners/mongodb-atlas/\" rel=\"nofollow noreferrer\">MongoDB Atlas on AWS</a></p>\n", "OwnerUserId": "11758843", "LastActivityDate": "2024-01-24T10:55:12.633", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "302542291", "PostHistoryTypeId": "2", "PostId": "77872429", "RevisionGUID": "6a52d2e7-c993-4c1c-8d2f-0a9bd380b87b", "CreationDate": "2024-01-24T10:55:12.633", "UserId": "11758843", "Text": "## Primary Solution\r\n\r\nI have been through this situation as well and then I finally decided to develop my own Terraform module to quickly deploy a secure, persistent, highly available, self healing, efficient, cost effective and self managed single-node or multi-node MongoDB NoSQL document database cluster on AWS ECS cluster with monitoring and alerting enabled.\r\n\r\nTerraform module can be found here: [MongoDB Cluster on AWS ECS - Terraform Module][1]\r\n\r\n#### Original questions:\r\n\r\n> I've been investigating creating my own mongodb cluster in AWS. Aws mongodb template provides some good starting points. However, it doesn't cover auto scaling or when a node goes down. For example, if I have 1 primary and 2 secondary nodes. And the primary goes down and auto scaling kicks in. How would I add the newly launched mongodb instance to the replica set?\r\n\r\n> If you look at the template, it uses an init.sh script to check if the node being launched is a primary node and waits for all other nodes to exist and creates a replica set with thier ip addresses on the primary. When the Replica set is configured initailly, all the nodes already exist.\r\n\r\n#### Answer:\r\n\r\nThe solution that I developed is based on AWS ECS and AWS EC2 Autoscaling. So, whenever a task or container fails, a new task is created automatically and whenever an AWS EC2 instance (node) goes down, it creates a new. It is developed in a way that the node can rejoin the cluster again. And yes, you can create up to 3 nodes in it. So, yes, it is highly available and self healing as well. You just need create a replica set after logging into one of the nodes using AWS SSM session manager and that's it. If the node goes down or the container fails, it will automatically fill up the replica set again. Even if you lose the whole cluster, you can reconfigure. I have shared some instructions in the code on that part but that is highly unlikely to happen.\r\n\r\n#### Another original question:\r\n\r\n> Not only that, but my node app uses mongoose. Part of the database connection allows you to specify multiple nodes. How would I keep track of what's currently up and running (I guess I could use DynamoDB but not sure).\r\n\r\n#### Answer:\r\n\r\nWith my solution, you can keep track using AWS ECS or AWS CloudWatch to make sure everything is up and running fine. I have set AWS CloudWatch alarms as well to send alerts when the utilization of CPU, Memory and Disk Space goes beyond 85%. There is a a parameter created on AWS SSM Parameter Store that has all the nodes URLs so you can use them to connect to the cluster. For example, in my case, I have used `mongosh` to connect as follows: `mongosh \"mongodb://[USERNAME]:[PASSWORD]@[ENVIRONMENT_NAME]-mongodb1.[AWS_ROUTE_53_PRIVATE_HOSTED_ZONE_NAME]:27017,[ENVIRONMENT_NAME]-mongodb2.[AWS_ROUTE_53_PRIVATE_HOSTED_ZONE_NAME]:27017,[ENVIRONMENT_NAME]-mongodb3.[AWS_ROUTE_53_PRIVATE_HOSTED_ZONE_NAME]:27017/admin?replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=true\"`\r\nYou can use mongoose as well.\r\n\r\nThere are many key highlights of my solution but I'm not sharing here to keep the answer precise and concise.\r\n\r\nMy solution is self managed and open source which gives us the ability to customize it as desired.\r\n\r\n## Secondary Solution\r\n\r\nThere is this partner solution available as well by MongoDB Atlas on the AWS Marketplace which can be tried. Link is here: [MongoDB Atlas on AWS][2]\r\n\r\n\r\n  [1]: https://github.com/abdullahkhawer/mongodb-cluster-on-aws-ecs\r\n  [2]: https://aws.amazon.com/solutions/partners/mongodb-atlas/", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "## Primary Solution I have been through this situation as well and then I finally decided to develop my own Terraform module to quickly deploy a secure, persistent, highly available, self healing, efficient, cost effective and self managed single-node or multi-node MongoDB NoSQL document database cluster on AWS ECS cluster with monitoring and alerting enabled. ", "keywords": ["cost", "efficient", "cluster"]}, {"source": "Text", "text": "Terraform module can be found here: [MongoDB Cluster on AWS ECS - Terraform Module][1] #### Original questions: > I've been investigating creating my own mongodb cluster in AWS. ", "keywords": ["cluster"]}, {"source": "Text", "text": "How would I add the newly launched mongodb instance to the replica set? ", "keywords": ["instance"]}, {"source": "Text", "text": "So, whenever a task or container fails, a new task is created automatically and whenever an AWS EC2 instance (node) goes down, it creates a new. ", "keywords": ["instance"]}, {"source": "Text", "text": "It is developed in a way that the node can rejoin the cluster again. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Even if you lose the whole cluster, you can reconfigure. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I have set AWS CloudWatch alarms as well to send alerts when the utilization of CPU, Memory and Disk Space goes beyond 85%. ", "keywords": ["cpu"]}, {"source": "Text", "text": "There is a a parameter created on AWS SSM Parameter Store that has all the nodes URLs so you can use them to connect to the cluster. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Link is here: [MongoDB Atlas on AWS][2] [1]: https://github.com/abdullahkhawer/mongodb-cluster-on-aws-ecs [2]: https://aws.amazon.com/solutions/partners/mongodb-atlas/", "keywords": ["cluster"]}]}], "filtered-sentences": [{"source": "Body", "text": "Primary Solution I have been through this situation as well and then I finally decided to develop my own Terraform module to quickly deploy a secure, persistent, highly available, self healing, efficient, cost effective and self managed single-node or multi-node MongoDB NoSQL document database cluster on AWS ECS cluster with monitoring and alerting enabled. ", "keywords": ["cost", "efficient", "cluster"]}, {"source": "Body", "text": "Terraform module can be found here: MongoDB Cluster on AWS ECS - Terraform Module Original questions: I've been investigating creating my own mongodb cluster in AWS. ", "keywords": ["cluster"]}, {"source": "Body", "text": "How would I add the newly launched mongodb instance to the replica set? ", "keywords": ["instance"]}, {"source": "Body", "text": "So, whenever a task or container fails, a new task is created automatically and whenever an AWS EC2 instance (node) goes down, it creates a new. ", "keywords": ["instance"]}, {"source": "Body", "text": "It is developed in a way that the node can rejoin the cluster again. ", "keywords": ["cluster"]}, {"source": "Body", "text": "Even if you lose the whole cluster, you can reconfigure. ", "keywords": ["cluster"]}, {"source": "Body", "text": "I have set AWS CloudWatch alarms as well to send alerts when the utilization of CPU, Memory and Disk Space goes beyond 85%. ", "keywords": ["cpu"]}, {"source": "Body", "text": "There is a a parameter created on AWS SSM Parameter Store that has all the nodes URLs so you can use them to connect to the cluster. ", "keywords": ["cluster"]}]}, {"Id": "34682862", "PostTypeId": "2", "ParentId": "30790038", "CreationDate": "2016-01-08T17:36:52.133", "Score": "45", "Body": "<p>This is a very good question and I went through this very painful journey myself recently.  I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others.</p>\n\n<p>I'm assuming that you're creating a MongoDB production cluster as follows: - </p>\n\n<ul>\n<li>3 config servers (micros/smalls instances can work here)</li>\n<li>At least 1 shard consisting of e.g. 2 (primary &amp; secondary) shard instances (minimum or large) with large disks configured for data / log / journal disks.</li>\n<li>arbiter machine for voting (micro probably OK).</li>\n</ul>\n\n<p>i.e. <a href=\"https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/\" rel=\"noreferrer\" title=\"MongoDB Production Cluster\">https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/</a></p>\n\n<p>Like yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (<a href=\"https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template\" rel=\"noreferrer\">https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template</a>) but to be honest it was far, far too complex i.e. it's 9,300 lines long and sets up multiple servers (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and it kept failing (e.g. after 15 mintues) which meant the servers all terminated again and I had to try again which was really frustrating / time consuming. </p>\n\n<p>The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. </p>\n\n<ol>\n<li><code>MongoDbConfigServer.template</code>   <em>(template to create config servers - run this 3 times)</em></li>\n<li><code>MongoDbShardedReplicaServer.template</code> <em>(template to create replica - run 2 times for each shard)</em></li>\n<li><code>MongoDbArbiterServer.template</code> <em>(template to create arbiter - run once for each shard)</em></li>\n</ol>\n\n<p><em>NOTE: templates available at <a href=\"https://github.com/adoreboard/aws-cloudformation-templates\" rel=\"noreferrer\">https://github.com/adoreboard/aws-cloudformation-templates</a></em></p>\n\n<p>The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g. the parameters for the replica server could include: -</p>\n\n<ul>\n<li><code>InstanceType</code> e.g. <code>t2.micro</code></li>\n<li><code>ReplicaSetName</code> e.g. <code>s1r</code>       <em>(shard 1 replica)</em></li>\n<li><code>ReplicaSetNumber</code> e.g. <code>2</code>       <em>(used with <code>ReplicaSetName</code> to create name e.g. name becomes <code>s1r2</code>)</em></li>\n<li><code>VpcId</code> e.g. <code>vpc-e4ad2b25</code>  <em>(not a real VPC obviously!)</em></li>\n<li><code>SubnetId</code> e.g. <code>subnet-2d39a157</code>   <em>(not a real subnet obviously!)</em></li>\n<li><code>GroupId</code> <em>(name of existing MongoDB group Id)</em></li>\n<li><code>Route53</code> <em>(boolean to add a record to an internal DNS - best practices)</em></li>\n<li><code>Route53HostedZone</code> <em>(if boolean is true then ID of internal DNS using Route53)</em></li>\n</ul>\n\n<p>The really cool thing about CloudFormation is that these custom parameters can have (a) a useful description for people running it, (b) special types (e.g. when running creates a prefiltered combobox so mistakes are harder to make) and (c) default values.  Here's an example: -</p>\n\n<pre><code>    \"Route53HostedZone\": {\n        \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\n        \"Type\": \"AWS::Route53::HostedZone::Id\",\n        \"Default\": \"YA3VWJWIX3FDC\"\n    },\n</code></pre>\n\n<p>This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing).</p>\n\n<p>As well as parameters, each of the 3 templates mentioned earlier have a <code>\"Resources\"</code> section which creates the instance.  We can do cool things via the <code>\"AWS::CloudFormation::Init\"</code> section also. e.g. </p>\n\n<pre><code>\"Resources\": {\n\n    \"MongoDbConfigServer\": {\n        \"Type\": \"AWS::EC2::Instance\",\n        \"Metadata\": {\n            \"AWS::CloudFormation::Init\": {\n                \"configSets\" : {\n                    \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\n                },\n</code></pre>\n\n<p>The <code>\"configSets\"</code> in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate things like DNS / Monitoring etc as much as possible.</p>\n\n<p>IMO, creating a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we have a <em>server-per-template</em> it's easy to build the MongoDB cluster up bit by bit.  </p>\n\n<p>My final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).</p>\n\n<p><a href=\"https://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\" rel=\"noreferrer\">https://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template</a></p>\n\n<p>In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time).  However, monitoring is very important, especially the disk sizes on the shard servers to alert you when disks are filling up (so you can either add a new shard to delete data). Monitoring can be achieved fairly easily using AWS CloudWatch metrics / alarms or using the MongoDB MMS service.</p>\n\n<p>If a node goes down e.g one of the replicas in a shard, then you can simply kill the server, recreate it using your CloudFormation template and the disks will sync across automatically.  This is my normal flow if an instance goes down and generally no re-configuration is necessary.  I've wasted far too many hours in the past trying to fix servers - sometimes lucky / sometimes not.  My backup strategy now is run a <code>mongodump</code> of the important collections of the database once a day via a <code>crontab</code>, <code>zip</code> up and upload to AWS S3.  This means if the nuclear option happens (complete database corruption) we can recreate the entire database and <code>mongorestore</code> in an hour or 2.</p>\n\n<p>However, if you create a new shard (because you're running out of space) configuration is necessary.  For example, if you are adding a new <strong>Shard 3</strong> you would create 2 replica nodes (e.g. primary with name => <code>mongo-s3r1</code> / secondary with name => <code>mongo-s3r2</code>) and 1 arbitor (e.g. with name <code>mongo-s3r-arb</code>) then you'd connect via a MongoDB shell to a <code>mongos</code> (MongoDB router) and run this command: -</p>\n\n<pre><code>sh.addShard(\"s3r/mongo-s3r1.internal.mycompany.com:27017,mongo-s3r2.internal.mycompany.com:27017\")\n</code></pre>\n\n<p><em><strong>NOTE:</strong> - This commands assumes you are using private DNS via Route53 (best practice).  You can simply use the private IPs of the 2 replicas in the <code>addShard</code> command but I have been very badly burned with this in the past (e.g. serveral months back all the AWS instances were restarted and new private IPs generated for all of them.  Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - whereas changing the IPs in Route53 takes a few seconds ... ;-)</em></p>\n\n<p>You could argue we should also add the <code>addShard</code> command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (<code>mongos</code>) and connect to that to run the <code>addShard</code> command.  Therefore I simply run this after the instances in a new MongoDB shard have been created.</p>\n\n<p>Anyways, that's my rather rambling thoughts on the matter.  The main thing is that once you have the templates in place your life becomes much easier and defo worth the effort!  Best of luck! :-)</p>\n", "OwnerUserId": "1692179", "LastEditorUserId": "1692179", "LastEditDate": "2017-06-21T15:46:32.850", "LastActivityDate": "2017-06-21T15:46:32.850", "CommentCount": "7", "ContentLicense": "CC BY-SA 3.0", "comments": [{"Id": "57185447", "PostId": "34682862", "Score": "0", "Text": "Thanks for the very detail explanation, I'll definatly give this a go at some point. I ended up going with a hosted solution for the time being because solving this problem isn't easy and can be time consuming, but you have some very good advice here that I would like to revisit. I must admit, the config provided by aws is very complicated.", "CreationDate": "2016-01-11T11:03:33.513", "UserId": "865024", "filtered-sentences": []}, {"Id": "57191316", "PostId": "34682862", "Score": "0", "Text": "There is advantages and disadvantages to a hosted solution (easy of use, quicker to deploy) vs hosting it yourself (power, control, potentional cheaper total cost of ownership etc).  I have done both and both are appropriate in different scenarios.  CloudFormation templates are tricky with a hefty learning curve (and AWS Dev-Ops in general) but well worth it!  The main advantages of using CloudFormation templates to bring up servers, install and configure software include (a) repeatability (b) infrastructure-as-code i.e. enables code reviews (c) reliability etc.", "CreationDate": "2016-01-11T13:32:41.907", "UserId": "1692179", "filtered-sentences": [{"source": "Text", "text": "There is advantages and disadvantages to a hosted solution (easy of use, quicker to deploy) vs hosting it yourself (power, control, potentional cheaper total cost of ownership etc). ", "keywords": ["cheap", "cost"]}]}, {"Id": "57847161", "PostId": "34682862", "Score": "0", "Text": "I ran into the same problems while trying to use the template provided by the AWS MongoDB Quickstart... it just took a long time and failed with little or no feedback. I like your approach, @bobmarksie it offers more control. Is there anywhere we can access the mentioned templates? (`MongoDbConfigServer.template`, `MongoDbShardedReplicaServer.template` and `MongoDbArbiterServer.template`)", "CreationDate": "2016-01-28T13:02:40.837", "UserId": "270052", "filtered-sentences": []}, {"Id": "57949446", "PostId": "34682862", "Score": "0", "Text": "Hi @monsieurBelbo - I've uploaded the templates to here - github.com/adoreboard/aws-cloudformation-templates. NOTE: there are no templates for creating (1) a VPC, (2) Security Groups, (3) Route53 private zone & (4) IAM Role for monitoring. If these don't already exist you can create these manually or via more CloudFormation templates. Otherwise, you can simply tweak the templates to suit your use case. I might do an indepth blog article on creating a MongoDB cluster this way when I get a free minute! If you've any tips for improvement I'll be happy to hear! Good luck.", "CreationDate": "2016-01-31T13:48:48.677", "UserId": "1692179", "filtered-sentences": [{"source": "Text", "text": "I might do an indepth blog article on creating a MongoDB cluster this way when I get a free minute! ", "keywords": ["cluster"]}]}, {"Id": "66956539", "PostId": "34682862", "Score": "0", "Text": "@bobmarksie , this is awesome!  I've researched a solution for the whole day that. And I decided to give up because I thought that `if I use AWS auto-scaling I never scale a MongoDB server or a cluster. Thereby auto-scaling is not an effective way to scale technical infrastructure -which has a database server`. And now your solution is in front of me!  On the other hand, I try to accomplish a secure AWS infrastructure for my app (as cheap as possible). I think your solution is not appropriate to a start-up that has not enough investment. Also there is no data yet. What do you think about that?", "CreationDate": "2016-10-03T14:07:33.753", "UserId": "3765109", "filtered-sentences": [{"source": "Text", "text": "And I decided to give up because I thought that `if I use AWS auto-scaling I never scale a MongoDB server or a cluster. ", "keywords": ["cluster"]}, {"source": "Text", "text": "On the other hand, I try to accomplish a secure AWS infrastructure for my app (as cheap as possible). ", "keywords": ["cheap"]}]}, {"Id": "66957311", "PostId": "34682862", "Score": "0", "Text": "@efkan - depends on how much money the startup has :-)  A single MongoDB machine may be totally fine for lots of startups.  It's only when the system must be totally reliable is when you need a production system (i.e. minimum 2 replicas, + 1 arbitor + 3 config servers).  This setup provides \"no single point of failure\".  However you can do this later i.e. when you have more money and the cost of annoying a customer with a broken experience could cost more than the AWS server costs.", "CreationDate": "2016-10-03T14:25:19.787", "UserId": "1692179", "filtered-sentences": [{"source": "Text", "text": "However you can do this later i.e. when you have more money and the cost of annoying a customer with a broken experience could cost more than the AWS server costs.", "keywords": ["cost"]}]}, {"Id": "66958428", "PostId": "34682862", "Score": "0", "Text": "Thanks @bobmarksie . Appearantly AWS AutoScaling is too early for me :)  Also I use Nginx reverse-proxy load balancer on top of my infrastructure. It might be complex to manage too.", "CreationDate": "2016-10-03T14:52:20.887", "UserId": "3765109", "filtered-sentences": []}], "history": [{"Id": "108012163", "PostHistoryTypeId": "2", "PostId": "34682862", "RevisionGUID": "f0b2af7d-c68b-4535-9bb6-85bd39a770fd", "CreationDate": "2016-01-08T17:36:52.133", "UserId": "1692179", "Text": "This is a very good question and I went through this very painful journey myself recently.  \r\n\r\nI'm assuming that you're creating a MongoDB production cluster: - \r\n\r\n* 3 config servers (micros/smalls instances can work here)\r\n* 2 or more shards each consisting of e.g. 2 shard servers (primary & secondary) and an (minimum medium or large instance with large disks configured for data / log / journal disks.\r\n* arbiter machine for voting (micro probably OK).  \r\ni.e.[https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1]\r\n\r\nLike yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template) but to be honest it was far, far too complex i.e. it's 9300 lines long and sets up multiple servers types (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and kept failing e.g. after 15 mintues which meant the servers all terminated again and I had to try again which was really frustrating.\r\n\r\nThe solution I went for in the end (which I'm super happy with) was to create separate templates for each template of MongoDB servers e.g. \r\n\r\n 1. MongoDbConfigServer.template\r\n 2. MongoDbShardedReplicaServer.template\r\n 3. MongoDbArbiterServer.template\r\n\r\nThe idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g.\r\n\r\n* `InstanceType` e.g. t2.micro\r\n* `ReplicaSetName` e.g. s1r       _(shard 1 replica)_\r\n* `VpcId` e.g. vpc-e4ad2b25  _(not a real VPC obviously!)_\r\n* `SubnetId` e.g. subnet-2d39a157   _(not a real VPC obviously!)_\r\n* `GroupId` _(name of existing MongoDB group Id)_\r\n* `Route53` _(boolean to add a record to an internal DNS - best practices)_\r\n* `Route53HostedZone` _(if boolean is true then ID of internal DNS using Route53)_\r\n\r\nThe really cool thing about CloudFormation is that these custom parameters can have (a) meaningful description, (b) special types and (3) default values.  Here's an example: -\r\n\r\n        \"Route53HostedZone\": {\r\n            \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\r\n            \"Type\": \"AWS::Route53::HostedZone::Id\",\r\n            \"Default\": \"YA3VWJWIX3FDC\"\r\n        },\r\n\r\nThis makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things here and there.\r\n\r\nAs well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance.  We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \r\n\r\n    \"Resources\": {\r\n\r\n        \"MongoDbConfigServer\": {\r\n            \"Type\": \"AWS::EC2::Instance\",\r\n            \"Metadata\": {\r\n                \"AWS::CloudFormation::Init\": {\r\n                    \"configSets\" : {\r\n                        \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\r\n                    },\r\n\r\nThe previous example shows the creating the config server isn't simply a matter of creating an AWS instance and installing MongoDB but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate as much as possible.\r\n\r\nIMO creating having a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we've separated out as server per template it's easy to build the MongoDB cluster up bit by bit.  \r\n\r\nMy final bit of advice is to copy what works for you from other open source MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).\r\n\r\nhttps://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\r\n\r\nAnyways, once you have the templates in place you're life is much easier and defo worth the effort!\r\n\r\nBest of luck! :-)\r\n\r\n  [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I'm assuming that you're creating a MongoDB production cluster: - * 3 config servers (micros/smalls instances can work here) * 2 or more shards each consisting of e.g. 2 shard servers (primary & secondary) and an (minimum medium or large instance with large disks configured for data / log / journal disks. ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "i.e.[https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1] ", "keywords": ["cluster"]}, {"source": "Text", "text": "The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Text", "text": "As well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance. ", "keywords": ["instance"]}, {"source": "Text", "text": "We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \"Resources\": { \"MongoDbConfigServer\": { \"Type\": \"AWS::EC2::Instance\", \"Metadata\": { \"AWS::CloudFormation::Init\": { \"configSets\" : { \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ] }, The previous example shows the creating the config server isn't simply a matter of creating an AWS instance and installing MongoDB but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Text", "text": "Also, because we've separated out as server per template it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Text", "text": "Best of luck! :-) [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "keywords": ["cluster"]}]}, {"Id": "108058997", "PostHistoryTypeId": "5", "PostId": "34682862", "RevisionGUID": "b7a0df74-6871-4393-9e6c-3018e5fecfdc", "CreationDate": "2016-01-09T11:49:09.190", "UserId": "1692179", "Comment": "Added some more detail and tried to answer the original question better.", "Text": "This is a very good question and I went through this very painful journey myself recently.  I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others.\r\n\r\nI'm assuming that you're creating a MongoDB production cluster as follows: - \r\n\r\n* 3 config servers (micros/smalls instances can work here)\r\n* At least 1 shards consisting of e.g. 2 (primary & secondary) shard instances (minimum medium or large) with large disks configured for data / log / journal disks.\r\n* arbiter machine for voting (micro probably OK).\r\n  \r\ni.e.[https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1]\r\n\r\nLike yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template) but to be honest it was far, far too complex i.e. it's 9,300 lines long and sets up multiple servers (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and it kept failing e.g. after 15 mintues, which meant the servers all terminated again and I had to try again which was really frustrating / time consuming. \r\n\r\nThe solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. \r\n\r\n 1. `MongoDbConfigServer.template`   _(template to create config servers - run this 3 times)_\r\n 2. `MongoDbShardedReplicaServer.template` _(template to create replica - run 2 times for each shard)_\r\n 3. `MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_\r\n\r\nThe idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g. the parameters for the replica server could include: -\r\n\r\n* `InstanceType` e.g. `t2.micro`\r\n* `ReplicaSetName` e.g. `s1r`       _(shard 1 replica)_\r\n* `ReplicaSetNumber` e.g. `2`       _(used with `ReplicaSetName` to create name e.g. name becomes `s1r2`)_\r\n* `VpcId` e.g. `vpc-e4ad2b25`  _(not a real VPC obviously!)_\r\n* `SubnetId` e.g. `subnet-2d39a157`   _(not a real VPC obviously!)_\r\n* `GroupId` _(name of existing MongoDB group Id)_\r\n* `Route53` _(boolean to add a record to an internal DNS - best practices)_\r\n* `Route53HostedZone` _(if boolean is true then ID of internal DNS using Route53)_\r\n\r\nThe really cool thing about CloudFormation is that these custom parameters can have (a) a useful description for people running it, (b) special types (e.g. when running creates a prefiltered combobox so mistakes are harder to make) and (c) default values.  Here's an example: -\r\n\r\n        \"Route53HostedZone\": {\r\n            \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\r\n            \"Type\": \"AWS::Route53::HostedZone::Id\",\r\n            \"Default\": \"YA3VWJWIX3FDC\"\r\n        },\r\n\r\nThis makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing).\r\n\r\nAs well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance.  We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \r\n\r\n    \"Resources\": {\r\n\r\n        \"MongoDbConfigServer\": {\r\n            \"Type\": \"AWS::EC2::Instance\",\r\n            \"Metadata\": {\r\n                \"AWS::CloudFormation::Init\": {\r\n                    \"configSets\" : {\r\n                        \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\r\n                    },\r\n\r\nThe `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate things like DNS / Monitoring etc as much as possible.\r\n\r\nIMO, creating a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit.  \r\n\r\nMy final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).\r\n\r\nhttps://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\r\n\r\nIn your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time).  However, monitoring is very important, especially the disk sizes on the shard servers to alert you when disks are filling up (so you can either add a new shard to delete data). Monitoring can be achieved fairly easily using AWS CloudWatch metrics / alarms or using the MongoDB MMS service.\r\n\r\nIf a node goes down e.g one of the replicas in a shard, then you can simply kill the server, recreate it using your CloudFormation template and the disks will sync across automatically.  This is my normal flow if an instance goes down and generally no re-configuration is necessary.  I've wasted far too many hours in the past trying to fix servers - sometimes lucky / sometimes not.  My backup strategy now is run a `mongodump` of the important collections of the database once a day via a `crontab`, `zip` up and upload to AWS S3.  This means if the nuclear option happens (complete database corruption) we can recreate the entire database and `mongorestore` in an hour or 2.\r\n\r\nHowever, if you create a new shard (because you're running out of space) configuration is necessary.  For example, if you are adding a new **Shard 3** you would create 2 replica nodes (e.g. primary with name => `mongo-s3r1` / secondary with name => `mongo-s3r2`) and 1 arbitor (e.g. with name `mongo-s3r-arb`) then you'd connect via a MongoDB shell to a `mongos` (MongoDB router) and run this command: -\r\n\r\n    sh.addShard(\"s3r/mongo-s3r1.internal.mycompany.com:27017,mongo-s3r2.internal.mycompany.com:27017\")\r\n\r\n_**NOTE:** - This commands assumes you're using private DNS via Route53 (best practice).  You can simply use the private IPs of the 2 replicas in the `addShard` command but I have been very badly burned with this in the past (e.g. server months back all the AWS servers were restarted and new IPs generated for all servers.  Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - where as changing the IPs in Route53 takes a few minutes ... ;-)_\r\n\r\nYou could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command.  Therefore I simply run this after the instances in a new MongoDB shard have been created.\r\n\r\nAnyways, that's my rather rambling thoughts on the matter.  The main thing is that once you have the templates in place you're life becomes much easier and defo worth the effort!  Best of luck! :-)\r\n\r\n  [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I'm assuming that you're creating a MongoDB production cluster as follows: - * 3 config servers (micros/smalls instances can work here) * At least 1 shards consisting of e.g. 2 (primary & secondary) shard instances (minimum medium or large) with large disks configured for data / log / journal disks. ", "keywords": ["cluster"]}, {"source": "Text", "text": "i.e.[https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1] ", "keywords": ["cluster"]}, {"source": "Text", "text": "The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. 1. ", "keywords": ["cluster"]}, {"source": "Text", "text": "`MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_ The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Here's an example: - \"Route53HostedZone\": { \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\", \"Type\": \"AWS::Route53::HostedZone::Id\", \"Default\": \"YA3VWJWIX3FDC\" }, This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing). ", "keywords": ["instance"]}, {"source": "Text", "text": "As well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance. ", "keywords": ["instance"]}, {"source": "Text", "text": "We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \"Resources\": { \"MongoDbConfigServer\": { \"Type\": \"AWS::EC2::Instance\", \"Metadata\": { \"AWS::CloudFormation::Init\": { \"configSets\" : { \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ] }, The `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Text", "text": "Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Text", "text": "In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time). ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "This is my normal flow if an instance goes down and generally no re-configuration is necessary. ", "keywords": ["instance"]}, {"source": "Text", "text": "Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - where as changing the IPs in Route53 takes a few minutes ... ;-)_ You could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command. ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "Best of luck! :-) [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "keywords": ["cluster"]}]}, {"Id": "109606400", "PostHistoryTypeId": "5", "PostId": "34682862", "RevisionGUID": "9f5afd55-addf-40b4-a0af-ebf23fa5244d", "CreationDate": "2016-01-29T14:56:32.027", "UserId": "1692179", "Comment": "added 91 characters in body", "Text": "This is a very good question and I went through this very painful journey myself recently.  I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others.\r\n\r\nI'm assuming that you're creating a MongoDB production cluster as follows: - \r\n\r\n* 3 config servers (micros/smalls instances can work here)\r\n* At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum medium or large) with large disks configured for data / log / journal disks.\r\n* arbiter machine for voting (micro probably OK).\r\n  \r\ni.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1]\r\n\r\nLike yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template) but to be honest it was far, far too complex i.e. it's 9,300 lines long and sets up multiple servers (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and it kept failing (e.g. after 15 mintues) which meant the servers all terminated again and I had to try again which was really frustrating / time consuming. \r\n\r\nThe solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. \r\n\r\n 1. `MongoDbConfigServer.template`   _(template to create config servers - run this 3 times)_\r\n 2. `MongoDbShardedReplicaServer.template` _(template to create replica - run 2 times for each shard)_\r\n 3. `MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_\r\n\r\n*NOTE: templates available at https://github.com/adoreboard/cloud-formation-templates*\r\n\r\nThe idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g. the parameters for the replica server could include: -\r\n\r\n* `InstanceType` e.g. `t2.micro`\r\n* `ReplicaSetName` e.g. `s1r`       _(shard 1 replica)_\r\n* `ReplicaSetNumber` e.g. `2`       _(used with `ReplicaSetName` to create name e.g. name becomes `s1r2`)_\r\n* `VpcId` e.g. `vpc-e4ad2b25`  _(not a real VPC obviously!)_\r\n* `SubnetId` e.g. `subnet-2d39a157`   _(not a real VPC obviously!)_\r\n* `GroupId` _(name of existing MongoDB group Id)_\r\n* `Route53` _(boolean to add a record to an internal DNS - best practices)_\r\n* `Route53HostedZone` _(if boolean is true then ID of internal DNS using Route53)_\r\n\r\nThe really cool thing about CloudFormation is that these custom parameters can have (a) a useful description for people running it, (b) special types (e.g. when running creates a prefiltered combobox so mistakes are harder to make) and (c) default values.  Here's an example: -\r\n\r\n        \"Route53HostedZone\": {\r\n            \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\r\n            \"Type\": \"AWS::Route53::HostedZone::Id\",\r\n            \"Default\": \"YA3VWJWIX3FDC\"\r\n        },\r\n\r\nThis makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing).\r\n\r\nAs well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance.  We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \r\n\r\n    \"Resources\": {\r\n\r\n        \"MongoDbConfigServer\": {\r\n            \"Type\": \"AWS::EC2::Instance\",\r\n            \"Metadata\": {\r\n                \"AWS::CloudFormation::Init\": {\r\n                    \"configSets\" : {\r\n                        \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\r\n                    },\r\n\r\nThe `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate things like DNS / Monitoring etc as much as possible.\r\n\r\nIMO, creating a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit.  \r\n\r\nMy final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).\r\n\r\nhttps://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\r\n\r\nIn your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time).  However, monitoring is very important, especially the disk sizes on the shard servers to alert you when disks are filling up (so you can either add a new shard to delete data). Monitoring can be achieved fairly easily using AWS CloudWatch metrics / alarms or using the MongoDB MMS service.\r\n\r\nIf a node goes down e.g one of the replicas in a shard, then you can simply kill the server, recreate it using your CloudFormation template and the disks will sync across automatically.  This is my normal flow if an instance goes down and generally no re-configuration is necessary.  I've wasted far too many hours in the past trying to fix servers - sometimes lucky / sometimes not.  My backup strategy now is run a `mongodump` of the important collections of the database once a day via a `crontab`, `zip` up and upload to AWS S3.  This means if the nuclear option happens (complete database corruption) we can recreate the entire database and `mongorestore` in an hour or 2.\r\n\r\nHowever, if you create a new shard (because you're running out of space) configuration is necessary.  For example, if you are adding a new **Shard 3** you would create 2 replica nodes (e.g. primary with name => `mongo-s3r1` / secondary with name => `mongo-s3r2`) and 1 arbitor (e.g. with name `mongo-s3r-arb`) then you'd connect via a MongoDB shell to a `mongos` (MongoDB router) and run this command: -\r\n\r\n    sh.addShard(\"s3r/mongo-s3r1.internal.mycompany.com:27017,mongo-s3r2.internal.mycompany.com:27017\")\r\n\r\n_**NOTE:** - This commands assumes you're using private DNS via Route53 (best practice).  You can simply use the private IPs of the 2 replicas in the `addShard` command but I have been very badly burned with this in the past (e.g. server months back all the AWS servers were restarted and new IPs generated for all servers.  Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - where as changing the IPs in Route53 takes a few minutes ... ;-)_\r\n\r\nYou could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command.  Therefore I simply run this after the instances in a new MongoDB shard have been created.\r\n\r\nAnyways, that's my rather rambling thoughts on the matter.  The main thing is that once you have the templates in place you're life becomes much easier and defo worth the effort!  Best of luck! :-)\r\n\r\n  [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I'm assuming that you're creating a MongoDB production cluster as follows: - * 3 config servers (micros/smalls instances can work here) * At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum medium or large) with large disks configured for data / log / journal disks. ", "keywords": ["cluster"]}, {"source": "Text", "text": "i.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1] ", "keywords": ["cluster"]}, {"source": "Text", "text": "The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. 1. ", "keywords": ["cluster"]}, {"source": "Text", "text": "`MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_ *NOTE: templates available at https://github.com/adoreboard/cloud-formation-templates* The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Here's an example: - \"Route53HostedZone\": { \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\", \"Type\": \"AWS::Route53::HostedZone::Id\", \"Default\": \"YA3VWJWIX3FDC\" }, This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing). ", "keywords": ["instance"]}, {"source": "Text", "text": "As well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance. ", "keywords": ["instance"]}, {"source": "Text", "text": "We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \"Resources\": { \"MongoDbConfigServer\": { \"Type\": \"AWS::EC2::Instance\", \"Metadata\": { \"AWS::CloudFormation::Init\": { \"configSets\" : { \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ] }, The `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Text", "text": "Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Text", "text": "My final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Text", "text": "In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time). ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "This is my normal flow if an instance goes down and generally no re-configuration is necessary. ", "keywords": ["instance"]}, {"source": "Text", "text": "Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - where as changing the IPs in Route53 takes a few minutes ... ;-)_ You could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command. ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "Best of luck! :-) [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "keywords": ["cluster"]}]}, {"Id": "109720236", "PostHistoryTypeId": "5", "PostId": "34682862", "RevisionGUID": "675b6a4d-6a1c-414e-b094-10ec1790b1ed", "CreationDate": "2016-01-31T13:48:12.723", "UserId": "1692179", "Comment": "Fixed correct GitHub URL", "Text": "This is a very good question and I went through this very painful journey myself recently.  I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others.\r\n\r\nI'm assuming that you're creating a MongoDB production cluster as follows: - \r\n\r\n* 3 config servers (micros/smalls instances can work here)\r\n* At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum medium or large) with large disks configured for data / log / journal disks.\r\n* arbiter machine for voting (micro probably OK).\r\n  \r\ni.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1]\r\n\r\nLike yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template) but to be honest it was far, far too complex i.e. it's 9,300 lines long and sets up multiple servers (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and it kept failing (e.g. after 15 mintues) which meant the servers all terminated again and I had to try again which was really frustrating / time consuming. \r\n\r\nThe solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. \r\n\r\n 1. `MongoDbConfigServer.template`   _(template to create config servers - run this 3 times)_\r\n 2. `MongoDbShardedReplicaServer.template` _(template to create replica - run 2 times for each shard)_\r\n 3. `MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_\r\n\r\n*NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates*\r\n\r\nThe idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g. the parameters for the replica server could include: -\r\n\r\n* `InstanceType` e.g. `t2.micro`\r\n* `ReplicaSetName` e.g. `s1r`       _(shard 1 replica)_\r\n* `ReplicaSetNumber` e.g. `2`       _(used with `ReplicaSetName` to create name e.g. name becomes `s1r2`)_\r\n* `VpcId` e.g. `vpc-e4ad2b25`  _(not a real VPC obviously!)_\r\n* `SubnetId` e.g. `subnet-2d39a157`   _(not a real VPC obviously!)_\r\n* `GroupId` _(name of existing MongoDB group Id)_\r\n* `Route53` _(boolean to add a record to an internal DNS - best practices)_\r\n* `Route53HostedZone` _(if boolean is true then ID of internal DNS using Route53)_\r\n\r\nThe really cool thing about CloudFormation is that these custom parameters can have (a) a useful description for people running it, (b) special types (e.g. when running creates a prefiltered combobox so mistakes are harder to make) and (c) default values.  Here's an example: -\r\n\r\n        \"Route53HostedZone\": {\r\n            \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\r\n            \"Type\": \"AWS::Route53::HostedZone::Id\",\r\n            \"Default\": \"YA3VWJWIX3FDC\"\r\n        },\r\n\r\nThis makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing).\r\n\r\nAs well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance.  We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \r\n\r\n    \"Resources\": {\r\n\r\n        \"MongoDbConfigServer\": {\r\n            \"Type\": \"AWS::EC2::Instance\",\r\n            \"Metadata\": {\r\n                \"AWS::CloudFormation::Init\": {\r\n                    \"configSets\" : {\r\n                        \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\r\n                    },\r\n\r\nThe `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate things like DNS / Monitoring etc as much as possible.\r\n\r\nIMO, creating a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit.  \r\n\r\nMy final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).\r\n\r\nhttps://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\r\n\r\nIn your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time).  However, monitoring is very important, especially the disk sizes on the shard servers to alert you when disks are filling up (so you can either add a new shard to delete data). Monitoring can be achieved fairly easily using AWS CloudWatch metrics / alarms or using the MongoDB MMS service.\r\n\r\nIf a node goes down e.g one of the replicas in a shard, then you can simply kill the server, recreate it using your CloudFormation template and the disks will sync across automatically.  This is my normal flow if an instance goes down and generally no re-configuration is necessary.  I've wasted far too many hours in the past trying to fix servers - sometimes lucky / sometimes not.  My backup strategy now is run a `mongodump` of the important collections of the database once a day via a `crontab`, `zip` up and upload to AWS S3.  This means if the nuclear option happens (complete database corruption) we can recreate the entire database and `mongorestore` in an hour or 2.\r\n\r\nHowever, if you create a new shard (because you're running out of space) configuration is necessary.  For example, if you are adding a new **Shard 3** you would create 2 replica nodes (e.g. primary with name => `mongo-s3r1` / secondary with name => `mongo-s3r2`) and 1 arbitor (e.g. with name `mongo-s3r-arb`) then you'd connect via a MongoDB shell to a `mongos` (MongoDB router) and run this command: -\r\n\r\n    sh.addShard(\"s3r/mongo-s3r1.internal.mycompany.com:27017,mongo-s3r2.internal.mycompany.com:27017\")\r\n\r\n_**NOTE:** - This commands assumes you're using private DNS via Route53 (best practice).  You can simply use the private IPs of the 2 replicas in the `addShard` command but I have been very badly burned with this in the past (e.g. server months back all the AWS servers were restarted and new IPs generated for all servers.  Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - where as changing the IPs in Route53 takes a few minutes ... ;-)_\r\n\r\nYou could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command.  Therefore I simply run this after the instances in a new MongoDB shard have been created.\r\n\r\nAnyways, that's my rather rambling thoughts on the matter.  The main thing is that once you have the templates in place you're life becomes much easier and defo worth the effort!  Best of luck! :-)\r\n\r\n  [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I'm assuming that you're creating a MongoDB production cluster as follows: - * 3 config servers (micros/smalls instances can work here) * At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum medium or large) with large disks configured for data / log / journal disks. ", "keywords": ["cluster"]}, {"source": "Text", "text": "i.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1] ", "keywords": ["cluster"]}, {"source": "Text", "text": "The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. 1. ", "keywords": ["cluster"]}, {"source": "Text", "text": "`MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_ *NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates* The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Here's an example: - \"Route53HostedZone\": { \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\", \"Type\": \"AWS::Route53::HostedZone::Id\", \"Default\": \"YA3VWJWIX3FDC\" }, This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing). ", "keywords": ["instance"]}, {"source": "Text", "text": "As well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance. ", "keywords": ["instance"]}, {"source": "Text", "text": "We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \"Resources\": { \"MongoDbConfigServer\": { \"Type\": \"AWS::EC2::Instance\", \"Metadata\": { \"AWS::CloudFormation::Init\": { \"configSets\" : { \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ] }, The `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Text", "text": "Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Text", "text": "My final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Text", "text": "https://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time). ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "This is my normal flow if an instance goes down and generally no re-configuration is necessary. ", "keywords": ["instance"]}, {"source": "Text", "text": "Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - where as changing the IPs in Route53 takes a few minutes ... ;-)_ You could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command. ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "Best of luck! :-) [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "keywords": ["cluster"]}]}, {"Id": "124623242", "PostHistoryTypeId": "5", "PostId": "34682862", "RevisionGUID": "3d8d5924-9625-4c72-bb2d-fe57e5a070a8", "CreationDate": "2016-08-10T14:23:43.807", "UserId": "1692179", "Comment": "minor updates", "Text": "This is a very good question and I went through this very painful journey myself recently.  I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others.\r\n\r\nI'm assuming that you're creating a MongoDB production cluster as follows: - \r\n\r\n* 3 config servers (micros/smalls instances can work here)\r\n* At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum or large) with large disks configured for data / log / journal disks.\r\n* arbiter machine for voting (micro probably OK).\r\n  \r\ni.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1]\r\n\r\nLike yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template) but to be honest it was far, far too complex i.e. it's 9,300 lines long and sets up multiple servers (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and it kept failing (e.g. after 15 mintues) which meant the servers all terminated again and I had to try again which was really frustrating / time consuming. \r\n\r\nThe solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. \r\n\r\n 1. `MongoDbConfigServer.template`   _(template to create config servers - run this 3 times)_\r\n 2. `MongoDbShardedReplicaServer.template` _(template to create replica - run 2 times for each shard)_\r\n 3. `MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_\r\n\r\n*NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates*\r\n\r\nThe idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g. the parameters for the replica server could include: -\r\n\r\n* `InstanceType` e.g. `t2.micro`\r\n* `ReplicaSetName` e.g. `s1r`       _(shard 1 replica)_\r\n* `ReplicaSetNumber` e.g. `2`       _(used with `ReplicaSetName` to create name e.g. name becomes `s1r2`)_\r\n* `VpcId` e.g. `vpc-e4ad2b25`  _(not a real VPC obviously!)_\r\n* `SubnetId` e.g. `subnet-2d39a157`   _(not a real subnet obviously!)_\r\n* `GroupId` _(name of existing MongoDB group Id)_\r\n* `Route53` _(boolean to add a record to an internal DNS - best practices)_\r\n* `Route53HostedZone` _(if boolean is true then ID of internal DNS using Route53)_\r\n\r\nThe really cool thing about CloudFormation is that these custom parameters can have (a) a useful description for people running it, (b) special types (e.g. when running creates a prefiltered combobox so mistakes are harder to make) and (c) default values.  Here's an example: -\r\n\r\n        \"Route53HostedZone\": {\r\n            \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\r\n            \"Type\": \"AWS::Route53::HostedZone::Id\",\r\n            \"Default\": \"YA3VWJWIX3FDC\"\r\n        },\r\n\r\nThis makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing).\r\n\r\nAs well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance.  We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \r\n\r\n    \"Resources\": {\r\n\r\n        \"MongoDbConfigServer\": {\r\n            \"Type\": \"AWS::EC2::Instance\",\r\n            \"Metadata\": {\r\n                \"AWS::CloudFormation::Init\": {\r\n                    \"configSets\" : {\r\n                        \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\r\n                    },\r\n\r\nThe `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate things like DNS / Monitoring etc as much as possible.\r\n\r\nIMO, creating a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit.  \r\n\r\nMy final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).\r\n\r\nhttps://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\r\n\r\nIn your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time).  However, monitoring is very important, especially the disk sizes on the shard servers to alert you when disks are filling up (so you can either add a new shard to delete data). Monitoring can be achieved fairly easily using AWS CloudWatch metrics / alarms or using the MongoDB MMS service.\r\n\r\nIf a node goes down e.g one of the replicas in a shard, then you can simply kill the server, recreate it using your CloudFormation template and the disks will sync across automatically.  This is my normal flow if an instance goes down and generally no re-configuration is necessary.  I've wasted far too many hours in the past trying to fix servers - sometimes lucky / sometimes not.  My backup strategy now is run a `mongodump` of the important collections of the database once a day via a `crontab`, `zip` up and upload to AWS S3.  This means if the nuclear option happens (complete database corruption) we can recreate the entire database and `mongorestore` in an hour or 2.\r\n\r\nHowever, if you create a new shard (because you're running out of space) configuration is necessary.  For example, if you are adding a new **Shard 3** you would create 2 replica nodes (e.g. primary with name => `mongo-s3r1` / secondary with name => `mongo-s3r2`) and 1 arbitor (e.g. with name `mongo-s3r-arb`) then you'd connect via a MongoDB shell to a `mongos` (MongoDB router) and run this command: -\r\n\r\n    sh.addShard(\"s3r/mongo-s3r1.internal.mycompany.com:27017,mongo-s3r2.internal.mycompany.com:27017\")\r\n\r\n_**NOTE:** - This commands assumes you are using private DNS via Route53 (best practice).  You can simply use the private IPs of the 2 replicas in the `addShard` command but I have been very badly burned with this in the past (e.g. serveral months back all the AWS instances were restarted and new private IPs generated for all of them.  Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - whereas changing the IPs in Route53 takes a few seconds ... ;-)_\r\n\r\nYou could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command.  Therefore I simply run this after the instances in a new MongoDB shard have been created.\r\n\r\nAnyways, that's my rather rambling thoughts on the matter.  The main thing is that once you have the templates in place your life becomes much easier and defo worth the effort!  Best of luck! :-)\r\n\r\n  [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I'm assuming that you're creating a MongoDB production cluster as follows: - * 3 config servers (micros/smalls instances can work here) * At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum or large) with large disks configured for data / log / journal disks. ", "keywords": ["cluster"]}, {"source": "Text", "text": "i.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1] ", "keywords": ["cluster"]}, {"source": "Text", "text": "The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. 1. ", "keywords": ["cluster"]}, {"source": "Text", "text": "`MongoDbArbiterServer.template` _(template to create replica - run once for each shard)_ *NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates* The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Here's an example: - \"Route53HostedZone\": { \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\", \"Type\": \"AWS::Route53::HostedZone::Id\", \"Default\": \"YA3VWJWIX3FDC\" }, This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing). ", "keywords": ["instance"]}, {"source": "Text", "text": "As well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance. ", "keywords": ["instance"]}, {"source": "Text", "text": "We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \"Resources\": { \"MongoDbConfigServer\": { \"Type\": \"AWS::EC2::Instance\", \"Metadata\": { \"AWS::CloudFormation::Init\": { \"configSets\" : { \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ] }, The `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Text", "text": "Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Text", "text": "My final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Text", "text": "In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time). ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "This is my normal flow if an instance goes down and generally no re-configuration is necessary. ", "keywords": ["instance"]}, {"source": "Text", "text": "Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - whereas changing the IPs in Route53 takes a few seconds ... ;-)_ You could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command. ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "Best of luck! :-) [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "keywords": ["cluster"]}]}, {"Id": "149893295", "PostHistoryTypeId": "5", "PostId": "34682862", "RevisionGUID": "0f0eb184-7bb7-4945-9123-ad7fb54b1d96", "CreationDate": "2017-06-21T15:46:32.850", "UserId": "1692179", "Comment": "edited body", "Text": "This is a very good question and I went through this very painful journey myself recently.  I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others.\r\n\r\nI'm assuming that you're creating a MongoDB production cluster as follows: - \r\n\r\n* 3 config servers (micros/smalls instances can work here)\r\n* At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum or large) with large disks configured for data / log / journal disks.\r\n* arbiter machine for voting (micro probably OK).\r\n  \r\ni.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1]\r\n\r\nLike yourself, I initially tried the AWS MongoDB CloudFormation template that you posted in the link (https://s3.amazonaws.com/quickstart-reference/mongodb/latest/templates/MongoDB-VPC.template) but to be honest it was far, far too complex i.e. it's 9,300 lines long and sets up multiple servers (i.e. replica shards, configs, arbitors, etc).  Running the CloudFormation template took ages and it kept failing (e.g. after 15 mintues) which meant the servers all terminated again and I had to try again which was really frustrating / time consuming. \r\n\r\nThe solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. \r\n\r\n 1. `MongoDbConfigServer.template`   _(template to create config servers - run this 3 times)_\r\n 2. `MongoDbShardedReplicaServer.template` _(template to create replica - run 2 times for each shard)_\r\n 3. `MongoDbArbiterServer.template` _(template to create arbiter - run once for each shard)_\r\n\r\n*NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates*\r\n\r\nThe idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor.  You can then add custom parameters into each of the templates e.g. the parameters for the replica server could include: -\r\n\r\n* `InstanceType` e.g. `t2.micro`\r\n* `ReplicaSetName` e.g. `s1r`       _(shard 1 replica)_\r\n* `ReplicaSetNumber` e.g. `2`       _(used with `ReplicaSetName` to create name e.g. name becomes `s1r2`)_\r\n* `VpcId` e.g. `vpc-e4ad2b25`  _(not a real VPC obviously!)_\r\n* `SubnetId` e.g. `subnet-2d39a157`   _(not a real subnet obviously!)_\r\n* `GroupId` _(name of existing MongoDB group Id)_\r\n* `Route53` _(boolean to add a record to an internal DNS - best practices)_\r\n* `Route53HostedZone` _(if boolean is true then ID of internal DNS using Route53)_\r\n\r\nThe really cool thing about CloudFormation is that these custom parameters can have (a) a useful description for people running it, (b) special types (e.g. when running creates a prefiltered combobox so mistakes are harder to make) and (c) default values.  Here's an example: -\r\n\r\n        \"Route53HostedZone\": {\r\n            \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\",\r\n            \"Type\": \"AWS::Route53::HostedZone::Id\",\r\n            \"Default\": \"YA3VWJWIX3FDC\"\r\n        },\r\n\r\nThis makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing).\r\n\r\nAs well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance.  We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \r\n\r\n    \"Resources\": {\r\n\r\n        \"MongoDbConfigServer\": {\r\n            \"Type\": \"AWS::EC2::Instance\",\r\n            \"Metadata\": {\r\n                \"AWS::CloudFormation::Init\": {\r\n                    \"configSets\" : {\r\n                        \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ]\r\n                    },\r\n\r\nThe `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc.  The idea is you want to automate things like DNS / Monitoring etc as much as possible.\r\n\r\nIMO, creating a template, and therefore a stack for each server has the very nice advantage of being able to replace a server extremely quickly via the CloudFormation web console.  Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit.  \r\n\r\nMy final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks).\r\n\r\nhttps://github.com/CaptainCodeman/mongo-aws-vpc/blob/master/src/templates/mongo-master.template\r\n\r\nIn your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time).  However, monitoring is very important, especially the disk sizes on the shard servers to alert you when disks are filling up (so you can either add a new shard to delete data). Monitoring can be achieved fairly easily using AWS CloudWatch metrics / alarms or using the MongoDB MMS service.\r\n\r\nIf a node goes down e.g one of the replicas in a shard, then you can simply kill the server, recreate it using your CloudFormation template and the disks will sync across automatically.  This is my normal flow if an instance goes down and generally no re-configuration is necessary.  I've wasted far too many hours in the past trying to fix servers - sometimes lucky / sometimes not.  My backup strategy now is run a `mongodump` of the important collections of the database once a day via a `crontab`, `zip` up and upload to AWS S3.  This means if the nuclear option happens (complete database corruption) we can recreate the entire database and `mongorestore` in an hour or 2.\r\n\r\nHowever, if you create a new shard (because you're running out of space) configuration is necessary.  For example, if you are adding a new **Shard 3** you would create 2 replica nodes (e.g. primary with name => `mongo-s3r1` / secondary with name => `mongo-s3r2`) and 1 arbitor (e.g. with name `mongo-s3r-arb`) then you'd connect via a MongoDB shell to a `mongos` (MongoDB router) and run this command: -\r\n\r\n    sh.addShard(\"s3r/mongo-s3r1.internal.mycompany.com:27017,mongo-s3r2.internal.mycompany.com:27017\")\r\n\r\n_**NOTE:** - This commands assumes you are using private DNS via Route53 (best practice).  You can simply use the private IPs of the 2 replicas in the `addShard` command but I have been very badly burned with this in the past (e.g. serveral months back all the AWS instances were restarted and new private IPs generated for all of them.  Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - whereas changing the IPs in Route53 takes a few seconds ... ;-)_\r\n\r\nYou could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command.  Therefore I simply run this after the instances in a new MongoDB shard have been created.\r\n\r\nAnyways, that's my rather rambling thoughts on the matter.  The main thing is that once you have the templates in place your life becomes much easier and defo worth the effort!  Best of luck! :-)\r\n\r\n  [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I'm assuming that you're creating a MongoDB production cluster as follows: - * 3 config servers (micros/smalls instances can work here) * At least 1 shard consisting of e.g. 2 (primary & secondary) shard instances (minimum or large) with large disks configured for data / log / journal disks. ", "keywords": ["cluster"]}, {"source": "Text", "text": "i.e. [https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/][1] ", "keywords": ["cluster"]}, {"source": "Text", "text": "The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. 1. ", "keywords": ["cluster"]}, {"source": "Text", "text": "`MongoDbArbiterServer.template` _(template to create arbiter - run once for each shard)_ *NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates* The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Text", "text": "Here's an example: - \"Route53HostedZone\": { \"Description\": \"Route 53 hosted zone for updating internal DNS (Only applicable if the parameter [ UpdateRoute53 ] = \\\"true\\\"\", \"Type\": \"AWS::Route53::HostedZone::Id\", \"Default\": \"YA3VWJWIX3FDC\" }, This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing). ", "keywords": ["instance"]}, {"source": "Text", "text": "As well as parameters, each of the 3 templates mentioned earlier have a `\"Resources\"` section which creates the instance. ", "keywords": ["instance"]}, {"source": "Text", "text": "We can do cool things via the `\"AWS::CloudFormation::Init\"` section also. e.g. \"Resources\": { \"MongoDbConfigServer\": { \"Type\": \"AWS::EC2::Instance\", \"Metadata\": { \"AWS::CloudFormation::Init\": { \"configSets\" : { \"Install\" : [ \"Metric-Uploading-Config\", \"Install-MongoDB\", \"Update-Route53\" ] }, The `\"configSets\"` in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Text", "text": "Also, because we have a _server-per-template_ it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Text", "text": "My final bit of advice on creating the templates would be to copy what works for you from other GitHub MongoDB CloudFormation templates e.g. I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Text", "text": "In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time). ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "This is my normal flow if an instance goes down and generally no re-configuration is necessary. ", "keywords": ["instance"]}, {"source": "Text", "text": "Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - whereas changing the IPs in Route53 takes a few seconds ... ;-)_ You could argue we should also add the `addShard` command to another CloudFormation template but IMO this adds unnecessary complexity because it has to know about a server which has a MongoDB router (`mongos`) and connect to that to run the `addShard` command. ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "Best of luck! :-) [1]: https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ \"MongoDB Production Cluster\"", "keywords": ["cluster"]}]}], "filtered-sentences": [{"source": "Body", "text": "I am writing a fairly extensive answer here in the hope that some of these thoughts of running a MongoDB cluster via CloudFormation are useful to others. ", "keywords": ["cluster"]}, {"source": "Body", "text": "I'm assuming that you're creating a MongoDB production cluster as follows: - 3 config servers (micros/smalls instances can work here) ", "keywords": ["cluster"]}, {"source": "Body", "text": "i.e. https://docs.mongodb.org/manual/core/sharded-cluster-architectures-production/ ", "keywords": ["cluster"]}, {"source": "Body", "text": "The solution I went for in the end (which I'm super happy with) was to create separate templates for each type of MongoDB server in the cluster e.g. MongoDbConfigServer.template (template to create config servers - run this 3 times) MongoDbShardedReplicaServer.template (template to create replica - run 2 times for each shard) MongoDbArbiterServer.template (template to create arbiter - run once for each shard) NOTE: templates available at https://github.com/adoreboard/aws-cloudformation-templates ", "keywords": ["cluster"]}, {"source": "Body", "text": "The idea then is to bring up each server in the cluster individually i.e. 3 config servers, 2 sharded replica servers (for 1 shard) and an arbitor. ", "keywords": ["cluster"]}, {"source": "Body", "text": "Here's an example: - This makes running the CloudFormation template an absolute breeze as a lot of the time we can rely on the default values and only tweak a couple of things depending on the server instance we're creating (or replacing). ", "keywords": ["instance"]}, {"source": "Body", "text": "As well as parameters, each of the 3 templates mentioned earlier have a \"Resources\" section which creates the instance. ", "keywords": ["instance"]}, {"source": "Body", "text": "The \"configSets\" in the previous example shows that creating a MongoDB server isn't simply a matter of creating an AWS instance and installing MongoDB on it but also we can (a) install CloudWatch disk / memory metrics (b) Update Route53 DNS etc. ", "keywords": ["instance"]}, {"source": "Body", "text": "Also, because we have a server-per-template it's easy to build the MongoDB cluster up bit by bit. ", "keywords": ["cluster"]}, {"source": "Body", "text": "I used the following to create the replica servers to use RAID10 (instead of the massively more expensive AWS provisioned IOPS disks). ", "keywords": ["expense"]}, {"source": "Body", "text": "In your question you mentioned auto-scaling - my preference would be to add a shard / replace a broken instance manually (auto-scaling makes sense with web containers e.g. Tomcat / Apache but a MongoDB cluster should really grow slowly over time). ", "keywords": ["instance", "cluster"]}, {"source": "Body", "text": "This is my normal flow if an instance goes down and generally no re-configuration is necessary. ", "keywords": ["instance"]}, {"source": "Body", "text": "Fixing the MongoDB cluster took me 2 days as I had to reconfigure everything manually - whereas changing the IPs in Route53 takes a few seconds ... ;-) ", "keywords": ["cluster", "change"]}]}], "contains-topic": true, "filtered-sentences": [{"source": "Title", "text": "Mongodb cluster with aws cloud formation and auto scaling", "keywords": ["cluster"]}, {"source": "Body", "text": "I've been investigating creating my own mongodb cluster in AWS. ", "keywords": ["cluster"]}, {"source": "Body", "text": "How would I add the newly launched mongodb instance to the replica set? ", "keywords": ["instance"]}, {"source": "Body", "text": "What's the usual flow if an instance goes down? ", "keywords": ["instance"]}]}
{"Id": "50059198", "PostTypeId": "1", "CreationDate": "2018-04-27T09:20:17.387", "Score": "2", "ViewCount": "1198", "Body": "<p>I have created CloudFormation template that creates ECS service and task and has autoscaling for tasks. It is pretty basic - if MemoruUtilization for tasks reaches certain value then add 1 task and vice verse. Here are some of the most relevant parts form template.</p>\n\n<pre><code>  EcsTd:\n    Type: AWS::ECS::TaskDefinition\n    DependsOn: LogGroup\n    Properties:\n      Family: !Sub ${EnvironmentName}-${PlatformName}-${Type}\n      ContainerDefinitions:\n      - Name: !Sub ${EnvironmentName}-${PlatformName}-${Type}\n        Image: !Sub ${AWS::AccountId}.dkr.ecr.{AWS::Region}.amazonaws.com/${PlatformName}:${ImageVersion}\n        Environment:\n        - Name: APP_ENV\n          Value: !If [isProd, \"production\", \"staging\"]\n        - Name: APP_DEBUG\n          Value: \"false\"\n        ...\n\n    PortMappings:\n    - ContainerPort: 80\n      HostPort: 0\n    Memory: !Ref Memory\n    Essential: true\n  EcsService:\n    Type: AWS::ECS::Service\n    DependsOn: WaitForLoadBalancerListenerRulesCondition\n    Properties:\n      ServiceName: !Sub ${EnvironmentName}-${PlatformName}-${Type}\n      Cluster:\n        Fn::ImportValue: !Sub ${EnvironmentName}-ECS-${Type}\n      DesiredCount: !Sub ${DesiredCount}\n      TaskDefinition: !Ref EcsTd\n      Role: \"learningEcsServiceRole\"\n      LoadBalancers:\n      - !If\n        - isWeb\n        - ContainerPort: 80\n          ContainerName: !Sub ${EnvironmentName}-${PlatformName}-${Type}\n          TargetGroupArn: !Ref AlbTargetGroup\n        - !Ref AWS::NoValue\n  ServiceScalableTarget:\n    Type: \"AWS::ApplicationAutoScaling::ScalableTarget\"\n    Properties:\n      MaxCapacity: !Sub ${MaxCount}\n      MinCapacity: !Sub ${MinCount}\n      ResourceId: !Join\n      - /\n      - - service\n        - !Sub ${EnvironmentName}-${Type}\n        - !GetAtt EcsService.Name\n      RoleARN: arn:aws:iam::645618565575:role/learningEcsServiceRole\n      ScalableDimension: ecs:service:DesiredCount\n      ServiceNamespace: ecs\n\n  ServiceScaleOutPolicy:\n    Type : \"AWS::ApplicationAutoScaling::ScalingPolicy\"\n    Properties:\n      PolicyName: !Sub ${EnvironmentName}-${PlatformName}-${Type}- ScaleOutPolicy\n      PolicyType: StepScaling\n      ScalingTargetId: !Ref ServiceScalableTarget\n      StepScalingPolicyConfiguration:\n        AdjustmentType: ChangeInCapacity\n        Cooldown: 1800\n        MetricAggregationType: Average\n        StepAdjustments:\n        - MetricIntervalLowerBound: 0\n          ScalingAdjustment: 1\n  MemoryScaleOutAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub ${EnvironmentName}-${PlatformName}-${Type}-MemoryOver70PercentAlarm\n      AlarmDescription: Alarm if memory utilization greater than 70% of reserved memory\n      Namespace: AWS/ECS\n      MetricName: MemoryUtilization\n      Dimensions:\n      - Name: ClusterName\n        Value: !Sub ${EnvironmentName}-${Type}\n      - Name: ServiceName\n        Value: !GetAtt EcsService.Name\n      Statistic: Maximum\n      Period: '60'\n      EvaluationPeriods: '1'\n      Threshold: '70'\n      ComparisonOperator: GreaterThanThreshold\n      AlarmActions:\n      - !Ref ServiceScaleOutPolicy\n      - !Ref EmailNotification\n\n  ...\n</code></pre>\n\n<p>So when ever task starts to run out of memory we'll add new task. However at some point we'll reach the limit how much memory are available in out cluster.</p>\n\n<p>So for example is Cluster consists of one t2.small instance then we have 2Gb RAM. A small amount of that is used by ECS task running in instance so we have less then 2GB RAM. If we set the value of Task's memory to 512Mb then we can put only 3 tasks in that cluster unless we scale up the cluster.</p>\n\n<p>By default ECS service has MemoryReservation metrics that can be used for autoscaling cluster. We would tell that when MemoryReservation in more then 75% then add 1 instance to cluster. That's relatively easy.</p>\n\n<pre><code>EcsCluster:\n    Type: AWS::ECS::Cluster\n    Properties:\n      ClusterName: !Sub ${EnvironmentName}-${Type}\n  SgEcsHost:\n    ...\n  ECSLaunchConfiguration:\n    Type: AWS::AutoScaling::LaunchConfiguration\n    Properties:\n      ImageId: !FindInMap [AWSRegionToAMI, !Ref 'AWS::Region', AMIID]\n      InstanceType: !Ref InstanceType\n      SecurityGroups: [ !Ref SgEcsHost ]\n      AssociatePublicIpAddress: true\n      IamInstanceProfile: \"ecsInstanceRole\"\n      KeyName: !Ref KeyName\n      UserData:\n        Fn::Base64: !Sub |\n          #!/bin/bash\n          echo ECS_CLUSTER=${EnvironmentName}-${Type} &gt;&gt; /etc/ecs/ecs.config\n  ECSAutoScalingGroup:\n    Type: AWS::AutoScaling::AutoScalingGroup\n    Properties:\n      VPCZoneIdentifier:\n      - Fn::ImportValue: !Sub ${EnvironmentName}-SubnetEC2AZ1\n      - Fn::ImportValue: !Sub ${EnvironmentName}-SubnetEC2AZ2\n      LaunchConfigurationName: !Ref ECSLaunchConfiguration\n      MinSize: !Ref AsgMinSize\n      MaxSize: !Ref AsgMaxSize\n      DesiredCapacity: !Ref AsgDesiredSize\n      Tags:\n      - Key: Name\n        Value: !Sub ${EnvironmentName}-ECS\n        PropagateAtLaunch: true\n  ScalePolicyUp:\n    Type: AWS::AutoScaling::ScalingPolicy\n    Properties:\n      AdjustmentType: ChangeInCapacity\n      AutoScalingGroupName:\n        Ref: ECSAutoScalingGroup\n      Cooldown: '1'\n      ScalingAdjustment: '1'\n  MemoryReservationAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      EvaluationPeriods: '1'\n      Statistic: Average\n      Threshold: '75'\n      AlarmDescription: Alarm if MemoryReservation is more then 75%\n      Period: '60'\n      AlarmActions:\n      - Ref: ScalePolicyUp\n      - Ref: EmailNotification\n      Namespace: AWS/EC2\n      Dimensions:\n      - Name: AutoScalingGroupName\n        Value:\n          Ref: ECSAutoScalingGroup\n      ComparisonOperator: GreaterThanThreshold\n      MetricName: MemoryReservation\n</code></pre>\n\n<p>However it does not make sense because that would happen when the third task is added so the new instance will be empty until 4th tasks is scaled. That means we'll be paying for instance that we don't use.</p>\n\n<p>I have noticed that when ECS service tries to add task to cluster where there is not enough free Memory I get </p>\n\n<blockquote>\n  <p>service Production-admin-worker was unable to place a task because no\n  container instance met all of its requirements. The closest matching\n  container-instance ################### has\n  insufficient memory available.</p>\n</blockquote>\n\n<p>In this example the template's parameters are:</p>\n\n<pre><code>EnvironmentName=Production\nPlatformName=Admin\nType=worker\n</code></pre>\n\n<p>Is it possible to create AWS::CloudWatch::Alarm that looks at ECS cluster events and looks for that particular pattern? The idea would be to scale up instance count in cluster using <code>AWS::AutoScaling::AutoScalingGroup</code> only when <code>AWS::ApplicationAutoScaling::ScalingPolicy</code> adds tasks that does not have space in cluster. And scale down the cluster when MemoryReservation is less then 25% (meaning that there are no tasks running there - <code>AWS::ApplicationAutoScaling::ScalingPolicy</code> has removed them).</p>\n", "OwnerUserId": "6822267", "LastActivityDate": "2018-05-03T13:53:42.933", "Title": "How to perform AWS CloudFormation autoscaling for ECS instance when cluster has insufficient memory available", "Tags": "|amazon-web-services|aws-cloudformation|amazon-cloudwatch|amazon-cloudwatch-metrics|", "AnswerCount": "1", "CommentCount": "0", "ContentLicense": "CC BY-SA 3.0", "history": [{"Id": "172221728", "PostHistoryTypeId": "2", "PostId": "50059198", "RevisionGUID": "3da5fed4-0f6e-448a-bd71-f8309b499959", "CreationDate": "2018-04-27T09:20:17.387", "UserId": "6822267", "Text": "I have created CloudFormation template that creates ECS service and task and has autoscaling for tasks. It is pretty basic - if MemoruUtilization for tasks reaches certain value then add 1 task and vice verse. Here are some of the most relevant parts form template.\r\n\r\n\r\n  \r\n\r\n      EcsTd:\r\n        Type: AWS::ECS::TaskDefinition\r\n        DependsOn: LogGroup\r\n        Properties:\r\n          Family: !Sub ${EnvironmentName}-${PlatformName}-${Type}\r\n          ContainerDefinitions:\r\n          - Name: !Sub ${EnvironmentName}-${PlatformName}-${Type}\r\n            Image: !Sub ${AWS::AccountId}.dkr.ecr.{AWS::Region}.amazonaws.com/${PlatformName}:${ImageVersion}\r\n            Environment:\r\n            - Name: APP_ENV\r\n              Value: !If [isProd, \"production\", \"staging\"]\r\n            - Name: APP_DEBUG\r\n              Value: \"false\"\r\n            ...\r\n    \r\n        PortMappings:\r\n        - ContainerPort: 80\r\n          HostPort: 0\r\n        Memory: !Ref Memory\r\n        Essential: true\r\n      EcsService:\r\n        Type: AWS::ECS::Service\r\n        DependsOn: WaitForLoadBalancerListenerRulesCondition\r\n        Properties:\r\n          ServiceName: !Sub ${EnvironmentName}-${PlatformName}-${Type}\r\n          Cluster:\r\n            Fn::ImportValue: !Sub ${EnvironmentName}-ECS-${Type}\r\n          DesiredCount: !Sub ${DesiredCount}\r\n          TaskDefinition: !Ref EcsTd\r\n          Role: \"learningEcsServiceRole\"\r\n          LoadBalancers:\r\n          - !If\r\n            - isWeb\r\n            - ContainerPort: 80\r\n              ContainerName: !Sub ${EnvironmentName}-${PlatformName}-${Type}\r\n              TargetGroupArn: !Ref AlbTargetGroup\r\n            - !Ref AWS::NoValue\r\n      ServiceScalableTarget:\r\n        Type: \"AWS::ApplicationAutoScaling::ScalableTarget\"\r\n        Properties:\r\n          MaxCapacity: !Sub ${MaxCount}\r\n          MinCapacity: !Sub ${MinCount}\r\n          ResourceId: !Join\r\n          - /\r\n          - - service\r\n            - !Sub ${EnvironmentName}-${Type}\r\n            - !GetAtt EcsService.Name\r\n          RoleARN: arn:aws:iam::645618565575:role/learningEcsServiceRole\r\n          ScalableDimension: ecs:service:DesiredCount\r\n          ServiceNamespace: ecs\r\n    \r\n      ServiceScaleOutPolicy:\r\n        Type : \"AWS::ApplicationAutoScaling::ScalingPolicy\"\r\n        Properties:\r\n          PolicyName: !Sub ${EnvironmentName}-${PlatformName}-${Type}- ScaleOutPolicy\r\n          PolicyType: StepScaling\r\n          ScalingTargetId: !Ref ServiceScalableTarget\r\n          StepScalingPolicyConfiguration:\r\n            AdjustmentType: ChangeInCapacity\r\n            Cooldown: 1800\r\n            MetricAggregationType: Average\r\n            StepAdjustments:\r\n            - MetricIntervalLowerBound: 0\r\n              ScalingAdjustment: 1\r\n      MemoryScaleOutAlarm:\r\n        Type: AWS::CloudWatch::Alarm\r\n        Properties:\r\n          AlarmName: !Sub ${EnvironmentName}-${PlatformName}-${Type}-MemoryOver70PercentAlarm\r\n          AlarmDescription: Alarm if memory utilization greater than 70% of reserved memory\r\n          Namespace: AWS/ECS\r\n          MetricName: MemoryUtilization\r\n          Dimensions:\r\n          - Name: ClusterName\r\n            Value: !Sub ${EnvironmentName}-${Type}\r\n          - Name: ServiceName\r\n            Value: !GetAtt EcsService.Name\r\n          Statistic: Maximum\r\n          Period: '60'\r\n          EvaluationPeriods: '1'\r\n          Threshold: '70'\r\n          ComparisonOperator: GreaterThanThreshold\r\n          AlarmActions:\r\n          - !Ref ServiceScaleOutPolicy\r\n          - !Ref EmailNotification\r\n    \r\n      ...\r\n\r\n\r\nSo when ever task starts to run out of memory we'll add new task. However at some point we'll reach the limit how much memory are available in out cluster.\r\n\r\nSo for example is Cluster consists of one t2.small instance then we have 2Gb RAM. A small amount of that is used by ECS task running in instance so we have less then 2GB RAM. If we set the value of Task's memory to 512Mb then we can put only 3 tasks in that cluster unless we scale up the cluster.\r\n\r\nBy default ECS service has MemoryReservation metrics that can be used for autoscaling cluster. We would tell that when MemoryReservation in more then 75% then add 1 instance to cluster. That's relatively easy.\r\n\r\n    EcsCluster:\r\n        Type: AWS::ECS::Cluster\r\n        Properties:\r\n          ClusterName: !Sub ${EnvironmentName}-${Type}\r\n      SgEcsHost:\r\n        ...\r\n      ECSLaunchConfiguration:\r\n        Type: AWS::AutoScaling::LaunchConfiguration\r\n        Properties:\r\n          ImageId: !FindInMap [AWSRegionToAMI, !Ref 'AWS::Region', AMIID]\r\n          InstanceType: !Ref InstanceType\r\n          SecurityGroups: [ !Ref SgEcsHost ]\r\n          AssociatePublicIpAddress: true\r\n          IamInstanceProfile: \"ecsInstanceRole\"\r\n          KeyName: !Ref KeyName\r\n          UserData:\r\n            Fn::Base64: !Sub |\r\n              #!/bin/bash\r\n              echo ECS_CLUSTER=${EnvironmentName}-${Type} >> /etc/ecs/ecs.config\r\n      ECSAutoScalingGroup:\r\n        Type: AWS::AutoScaling::AutoScalingGroup\r\n        Properties:\r\n          VPCZoneIdentifier:\r\n          - Fn::ImportValue: !Sub ${EnvironmentName}-SubnetEC2AZ1\r\n          - Fn::ImportValue: !Sub ${EnvironmentName}-SubnetEC2AZ2\r\n          LaunchConfigurationName: !Ref ECSLaunchConfiguration\r\n          MinSize: !Ref AsgMinSize\r\n          MaxSize: !Ref AsgMaxSize\r\n          DesiredCapacity: !Ref AsgDesiredSize\r\n          Tags:\r\n          - Key: Name\r\n            Value: !Sub ${EnvironmentName}-ECS\r\n            PropagateAtLaunch: true\r\n      ScalePolicyUp:\r\n        Type: AWS::AutoScaling::ScalingPolicy\r\n        Properties:\r\n          AdjustmentType: ChangeInCapacity\r\n          AutoScalingGroupName:\r\n            Ref: ECSAutoScalingGroup\r\n          Cooldown: '1'\r\n          ScalingAdjustment: '1'\r\n      MemoryReservationAlarm:\r\n        Type: AWS::CloudWatch::Alarm\r\n        Properties:\r\n          EvaluationPeriods: '1'\r\n          Statistic: Average\r\n          Threshold: '75'\r\n          AlarmDescription: Alarm if MemoryReservation is more then 75%\r\n          Period: '60'\r\n          AlarmActions:\r\n          - Ref: ScalePolicyUp\r\n          - Ref: EmailNotification\r\n          Namespace: AWS/EC2\r\n          Dimensions:\r\n          - Name: AutoScalingGroupName\r\n            Value:\r\n              Ref: ECSAutoScalingGroup\r\n          ComparisonOperator: GreaterThanThreshold\r\n          MetricName: MemoryReservation\r\n\r\nHowever it does not make sense because that would happen when the third task is added so the new instance will be empty until 4th tasks is scaled. That means we'll be paying for instance that we don't use.\r\n\r\nI have noticed that when ECS service tries to add task to cluster where there is not enough free Memory I get \r\n\r\n> service Production-admin-worker was unable to place a task because no\r\n> container instance met all of its requirements. The closest matching\r\n> container-instance ################### has\r\n> insufficient memory available.\r\n\r\nIn this example the template's parameters are:\r\n\r\n    EnvironmentName=Production\r\n    PlatformName=Admin\r\n    Type=worker\r\n\r\nIs it possible to create AWS::CloudWatch::Alarm that looks at ECS cluster events and looks for that particular pattern? The idea would be to scale up instance count in cluster using `AWS::AutoScaling::AutoScalingGroup` only when `AWS::ApplicationAutoScaling::ScalingPolicy` adds tasks that does not have space in cluster. And scale down the cluster when MemoryReservation is less then 25% (meaning that there are no tasks running there - `AWS::ApplicationAutoScaling::ScalingPolicy` has removed them).", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "Here are some of the most relevant parts form template. EcsTd: Type: AWS::ECS::TaskDefinition DependsOn: LogGroup Properties: Family: !Sub ${EnvironmentName}-${PlatformName}-${Type} ContainerDefinitions: - Name: !Sub ${EnvironmentName}-${PlatformName}-${Type} Image: !Sub ${AWS::AccountId}.dkr.ecr.{AWS::Region}.amazonaws.com/${PlatformName}:${ImageVersion} Environment: - Name: APP_ENV Value: !If [isProd, \"production\", \"staging\"] - Name: APP_DEBUG Value: \"false\" ... PortMappings: - ContainerPort: 80 HostPort: 0 Memory: !Ref Memory Essential: true EcsService: Type: AWS::ECS::Service DependsOn: WaitForLoadBalancerListenerRulesCondition Properties: ServiceName: !Sub ${EnvironmentName}-${PlatformName}-${Type} Cluster: Fn::ImportValue: !Sub ${EnvironmentName}-ECS-${Type} DesiredCount: !Sub ${DesiredCount} TaskDefinition: !Ref EcsTd Role: \"learningEcsServiceRole\" LoadBalancers: - !If - isWeb - ContainerPort: 80 ContainerName: !Sub ${EnvironmentName}-${PlatformName}-${Type} TargetGroupArn: !Ref AlbTargetGroup - !Ref AWS::NoValue ServiceScalableTarget: Type: \"AWS::ApplicationAutoScaling::ScalableTarget\" Properties: MaxCapacity: !Sub ${MaxCount} MinCapacity: !Sub ${MinCount} ResourceId: !Join - / - - service - !Sub ${EnvironmentName}-${Type} - !GetAtt EcsService.Name RoleARN: arn:aws:iam::645618565575:role/learningEcsServiceRole ScalableDimension: ecs:service:DesiredCount ServiceNamespace: ecs ServiceScaleOutPolicy: Type : \"AWS::ApplicationAutoScaling::ScalingPolicy\" Properties: PolicyName: !Sub ${EnvironmentName}-${PlatformName}-${Type}- ScaleOutPolicy PolicyType: StepScaling ScalingTargetId: !Ref ServiceScalableTarget StepScalingPolicyConfiguration: AdjustmentType: ChangeInCapacity Cooldown: 1800 MetricAggregationType: Average StepAdjustments: - MetricIntervalLowerBound: 0 ScalingAdjustment: 1 MemoryScaleOutAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: !Sub ${EnvironmentName}-${PlatformName}-${Type}-MemoryOver70PercentAlarm AlarmDescription: Alarm if memory utilization greater than 70% of reserved memory Namespace: AWS/ECS MetricName: MemoryUtilization Dimensions: - Name: ClusterName Value: !Sub ${EnvironmentName}-${Type} - Name: ServiceName Value: !GetAtt EcsService.Name Statistic: Maximum Period: '60' EvaluationPeriods: '1' Threshold: '70' ComparisonOperator: GreaterThanThreshold AlarmActions: - !Ref ServiceScaleOutPolicy - !Ref EmailNotification ... ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "However at some point we'll reach the limit how much memory are available in out cluster. ", "keywords": ["cluster"]}, {"source": "Text", "text": "So for example is Cluster consists of one t2.small instance then we have 2Gb RAM. ", "keywords": ["instance", "ram", "cluster"]}, {"source": "Text", "text": "A small amount of that is used by ECS task running in instance so we have less then 2GB RAM. ", "keywords": ["instance", "ram"]}, {"source": "Text", "text": "If we set the value of Task's memory to 512Mb then we can put only 3 tasks in that cluster unless we scale up the cluster. ", "keywords": ["cluster"]}, {"source": "Text", "text": "By default ECS service has MemoryReservation metrics that can be used for autoscaling cluster. ", "keywords": ["cluster"]}, {"source": "Text", "text": "We would tell that when MemoryReservation in more then 75% then add 1 instance to cluster. ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "EcsCluster: Type: AWS::ECS::Cluster Properties: ClusterName: !Sub ${EnvironmentName}-${Type} SgEcsHost: ... ECSLaunchConfiguration: Type: AWS::AutoScaling::LaunchConfiguration Properties: ImageId: !FindInMap [AWSRegionToAMI, !Ref 'AWS::Region', AMIID] InstanceType: !Ref InstanceType SecurityGroups: [ !Ref SgEcsHost ] AssociatePublicIpAddress: true IamInstanceProfile: \"ecsInstanceRole\" KeyName: !Ref KeyName UserData: Fn::Base64: !Sub | #!/bin/bash echo ECS_CLUSTER=${EnvironmentName}-${Type} >> /etc/ecs/ecs.config ECSAutoScalingGroup: Type: AWS::AutoScaling::AutoScalingGroup Properties: VPCZoneIdentifier: - Fn::ImportValue: !Sub ${EnvironmentName}-SubnetEC2AZ1 - Fn::ImportValue: !Sub ${EnvironmentName}-SubnetEC2AZ2 LaunchConfigurationName: !Ref ECSLaunchConfiguration MinSize: !Ref AsgMinSize MaxSize: !Ref AsgMaxSize DesiredCapacity: !Ref AsgDesiredSize Tags: - Key: Name Value: !Sub ${EnvironmentName}-ECS PropagateAtLaunch: true ScalePolicyUp: Type: AWS::AutoScaling::ScalingPolicy Properties: AdjustmentType: ChangeInCapacity AutoScalingGroupName: Ref: ECSAutoScalingGroup Cooldown: '1' ScalingAdjustment: '1' MemoryReservationAlarm: Type: AWS::CloudWatch::Alarm Properties: EvaluationPeriods: '1' Statistic: Average Threshold: '75' AlarmDescription: Alarm if MemoryReservation is more then 75% Period: '60' AlarmActions: - Ref: ScalePolicyUp - Ref: EmailNotification Namespace: AWS/EC2 Dimensions: - Name: AutoScalingGroupName Value: Ref: ECSAutoScalingGroup ComparisonOperator: GreaterThanThreshold MetricName: MemoryReservation ", "keywords": ["cluster", "change"]}, {"source": "Text", "text": "However it does not make sense because that would happen when the third task is added so the new instance will be empty until 4th tasks is scaled. ", "keywords": ["instance"]}, {"source": "Text", "text": "That means we'll be paying for instance that we don't use. ", "keywords": ["instance"]}, {"source": "Text", "text": "I have noticed that when ECS service tries to add task to cluster where there is not enough free Memory I get > service Production-admin-worker was unable to place a task because no > container instance met all of its requirements. ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "The closest matching > container-instance ################### has > insufficient memory available. ", "keywords": ["instance"]}, {"source": "Text", "text": "Is it possible to create AWS::CloudWatch::Alarm that looks at ECS cluster events and looks for that particular pattern? ", "keywords": ["cluster"]}, {"source": "Text", "text": "The idea would be to scale up instance count in cluster using `AWS::AutoScaling::AutoScalingGroup` only when `AWS::ApplicationAutoScaling::ScalingPolicy` adds tasks that does not have space in cluster. ", "keywords": ["instance", "cluster"]}, {"source": "Text", "text": "And scale down the cluster when MemoryReservation is less then 25% (meaning that there are no tasks running there - `AWS::ApplicationAutoScaling::ScalingPolicy` has removed them).", "keywords": ["cluster"]}]}, {"Id": "172221729", "PostHistoryTypeId": "1", "PostId": "50059198", "RevisionGUID": "3da5fed4-0f6e-448a-bd71-f8309b499959", "CreationDate": "2018-04-27T09:20:17.387", "UserId": "6822267", "Text": "How to perform AWS CloudFormation autoscaling for ECS instance when cluster has insufficient memory available", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "How to perform AWS CloudFormation autoscaling for ECS instance when cluster has insufficient memory available", "keywords": ["instance", "cluster"]}]}, {"Id": "172221730", "PostHistoryTypeId": "3", "PostId": "50059198", "RevisionGUID": "3da5fed4-0f6e-448a-bd71-f8309b499959", "CreationDate": "2018-04-27T09:20:17.387", "UserId": "6822267", "Text": "|amazon-web-services|aws-cloudformation|amazon-cloudwatch|amazon-cloudwatch-metrics|", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}], "answers": [{"Id": "50156774", "PostTypeId": "2", "ParentId": "50059198", "CreationDate": "2018-05-03T13:53:42.933", "Score": "0", "Body": "<blockquote>\n  <p>That means we'll be paying for instance that we don't use.</p>\n</blockquote>\n\n<p>Either you pay for the extra/backup capacity in advance, or implement logic to retry the ones that failed due to low capacity.</p>\n\n<p>Couple of ways I can think of:</p>\n\n<ul>\n<li>You could create a custom script/lambda (<a href=\"https://forums.aws.amazon.com/thread.jspa?threadID=94984\" rel=\"nofollow noreferrer\">https://forums.aws.amazon.com/thread.jspa?threadID=94984</a>) that reports a metric say <code>load_factor</code> calculated as <code>number of tasks / number of instances</code> and then base an your auto scaling policy on that. Lambda can be triggered by a CW Rule.\n\n<ul>\n<li>You could also report this from your task implementation instead of a new custom lambda/script.</li>\n</ul></li>\n<li>Create <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\" rel=\"nofollow noreferrer\">a metric filter</a> that looks for a specific pattern in a log file/group and reports a metric. Then of course use this metric for scaling.</li>\n</ul>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\" rel=\"nofollow noreferrer\">From docs:</a></p>\n\n<blockquote>\n  <p>When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric.</p>\n</blockquote>\n", "OwnerUserId": "496289", "LastActivityDate": "2018-05-03T13:53:42.933", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "172635675", "PostHistoryTypeId": "2", "PostId": "50156774", "RevisionGUID": "2b113b17-701f-44c9-a84a-e1f42553d7b9", "CreationDate": "2018-05-03T13:53:42.933", "UserId": "496289", "Text": "\r\n\r\n> That means we'll be paying for instance that we don't use.\r\n\r\nEither you pay for the extra/backup capacity in advance, or implement logic to retry the ones that failed due to low capacity.\r\n\r\nCouple of ways I can think of:\r\n\r\n* You could create a custom script/lambda (https://forums.aws.amazon.com/thread.jspa?threadID=94984) that reports a metric say `load_factor` calculated as `number of tasks / number of instances` and then base an your auto scaling policy on that. Lambda can be triggered by a CW Rule.\r\n * You could also report this from your task implementation instead of a new custom lambda/script.\r\n* Create [a metric filter][1] that looks for a specific pattern in a log file/group and reports a metric. Then of course use this metric for scaling.\r\n\r\n[From docs:][1]\r\n> When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric.\r\n\r\n  [1]: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "That means we'll be paying for instance that we don't use. ", "keywords": ["instance"]}, {"source": "Text", "text": "Either you pay for the extra/backup capacity in advance, or implement logic to retry the ones that failed due to low capacity. ", "keywords": ["pay"]}, {"source": "Text", "text": "Couple of ways I can think of: * You could create a custom script/lambda (https://forums.aws.amazon.com/thread.jspa?threadID=94984) that reports a metric say `load_factor` calculated as `number of tasks / number of instances` and then base an your auto scaling policy on that. ", "keywords": ["policy"]}]}], "filtered-sentences": [{"source": "Body", "text": "That means we'll be paying for instance that we don't use. ", "keywords": ["instance"]}, {"source": "Body", "text": "Either you pay for the extra/backup capacity in advance, or implement logic to retry the ones that failed due to low capacity. ", "keywords": ["pay"]}, {"source": "Body", "text": "Couple of ways I can think of: You could create a custom script/lambda (https://forums.aws.amazon.com/thread.jspa?threadID=94984) that reports a metric say load_factor calculated as number of tasks / number of instances and then base an your auto scaling policy on that. ", "keywords": ["policy"]}]}], "contains-topic": true, "filtered-sentences": [{"source": "Title", "text": "How to perform AWS CloudFormation autoscaling for ECS instance when cluster has insufficient memory available", "keywords": ["instance", "cluster"]}, {"source": "Body", "text": "However at some point we'll reach the limit how much memory are available in out cluster. ", "keywords": ["cluster"]}, {"source": "Body", "text": "So for example is Cluster consists of one t2.small instance then we have 2Gb RAM. ", "keywords": ["instance", "ram", "cluster"]}, {"source": "Body", "text": "A small amount of that is used by ECS task running in instance so we have less then 2GB RAM. ", "keywords": ["instance", "ram"]}, {"source": "Body", "text": "If we set the value of Task's memory to 512Mb then we can put only 3 tasks in that cluster unless we scale up the cluster. ", "keywords": ["cluster"]}, {"source": "Body", "text": "By default ECS service has MemoryReservation metrics that can be used for autoscaling cluster. ", "keywords": ["cluster"]}, {"source": "Body", "text": "We would tell that when MemoryReservation in more then 75% then add 1 instance to cluster. ", "keywords": ["instance", "cluster"]}, {"source": "Body", "text": "However it does not make sense because that would happen when the third task is added so the new instance will be empty until 4th tasks is scaled. ", "keywords": ["instance"]}, {"source": "Body", "text": "That means we'll be paying for instance that we don't use. ", "keywords": ["instance"]}, {"source": "Body", "text": "I have noticed that when ECS service tries to add task to cluster where there is not enough free Memory I get service Production-admin-worker was unable to place a task because no container instance met all of its requirements. ", "keywords": ["instance", "cluster"]}, {"source": "Body", "text": "The closest matching container-instance ################### has insufficient memory available. ", "keywords": ["instance"]}, {"source": "Body", "text": "In this example the template's parameters are: Is it possible to create AWS::CloudWatch::Alarm that looks at ECS cluster events and looks for that particular pattern? ", "keywords": ["cluster"]}, {"source": "Body", "text": "The idea would be to scale up instance count in cluster using AWS::AutoScaling::AutoScalingGroup only when AWS::ApplicationAutoScaling::ScalingPolicy adds tasks that does not have space in cluster. ", "keywords": ["instance", "cluster"]}, {"source": "Body", "text": "And scale down the cluster when MemoryReservation is less then 25% (meaning that there are no tasks running there - AWS::ApplicationAutoScaling::ScalingPolicy has removed them).", "keywords": ["cluster"]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f13299-76d1-47bb-9413-2ef3d6e73bce",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae437d-72a4-408d-baf5-cb79894fa6b5",
   "metadata": {},
   "source": [
    "## Pre-processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff82d067-bbc5-4ef0-8598-31573b6ca467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import stanza\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from markdown import markdown\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5f4b0-9374-4a9c-86de-e46f44eae2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = STOPWORDS.union((\n",
    "    \"var\", \"variable\", \"computed\", \"costa\", \"botocore\", \"version\", \"step\",\n",
    "    \"support\", \"source\", \"hashicorp\", \"service\", \"branch\", \"pull\", \"merge\", \"issue\",\n",
    "    \"pr\", \"galoy-pay\", \"bumped\", \"add\", \"payload\", \"boto\", \"accurics\", \"hana\",\n",
    "    \"bump\", \"added\", \"latest\", \"update\", \"github\", \"test\", \"sourced\",\n",
    "    \"instead\", \"use\", \"plan\", \"updates\", \"diff\", \"bump-galoy-pay-image\", \"draft\",\n",
    "    \"iam\", \"i'm\", \"v1\", \"apply\", \"fix\", \"fixes\", \"kvo\", \"needed\", \"tco\", \"create\",\n",
    "    \"run\", \"code\", \"feat\", \"lambda\", \"need\", \"link\", \"project\", \"new\", \"change\",\n",
    "    \"they're\",\"SAM_template\",\"ABC_Lambda\", \"AWS\", \"Cloudformation\", \"removepermission\",\n",
    "    \"cloudformation\", \"how\", \"like\"\n",
    "))\n",
    "\n",
    "UPOS = ('PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV')\n",
    "nlp_pipeline = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def load_json_files(folder_path):\n",
    "    data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                data.append(json.load(file))\n",
    "    return data\n",
    "\n",
    "def clean_markup(doc):\n",
    "    # Convert Markdown to HTML markup\n",
    "    clean_doc = markdown(doc,extensions=['fenced_code'])\n",
    "    clean_doc = BeautifulSoup(clean_doc)\n",
    "    # Remove unwanted content\n",
    "    for s in clean_doc.select('code'):\n",
    "        s.extract()\n",
    "    for s in clean_doc.select('pre'):\n",
    "        s.extract()\n",
    "    for s in clean_doc.select('blockquotes'):\n",
    "        s.extract()\n",
    "    # Remove HTML markup\n",
    "    clean_doc = ''.join(clean_doc.findAll(text=True))\n",
    "    # Remove URLs\n",
    "    clean_doc = re.sub(r'\\S*https?:\\S*', '', clean_doc, flags=re.MULTILINE)\n",
    "    \n",
    "    return clean_doc\n",
    "\n",
    "def prepare_document_so(doc):\n",
    "    clean_doc = clean_markup(doc)\n",
    "    tokens = []\n",
    "    for token in nlp_pipeline(clean_doc).iter_tokens():\n",
    "        token_dict = token.to_dict()[0]\n",
    "        if 'upos' in token_dict and token_dict['upos'] in UPOS and token_dict['text'] not in STOPWORDS:\n",
    "            tokens.append(token_dict['lemma'])\n",
    "        else:\n",
    "            print(f\"Token missing 'upos': {token_dict}\")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def prepare_corpus_so(documents):\n",
    "    corpus = []\n",
    "    total_docs = len(documents)\n",
    "    for i in range(total_docs):\n",
    "        print(f\"SO post:{i}\\n\")\n",
    "        if isinstance(documents[i], dict):\n",
    "            if 'Body' in documents[i]:\n",
    "                tokens_body = prepare_document_so(documents[i]['Body'])\n",
    "                corpus.append(tokens_body)\n",
    "                print(f\"body tokens: {tokens_body}\")\n",
    "            if 'Title' in documents[i]:\n",
    "                tokens_title = prepare_document_so(documents[i]['Title'])\n",
    "                corpus.append(tokens_title)\n",
    "                print(f\"title tokens: {tokens_title}\")\n",
    "            if 'comments' in documents[i]:\n",
    "                for comment in documents[i]['comments']:\n",
    "                    tokens_comments = prepare_document_so(comment['Text'])\n",
    "                    corpus.append(tokens_comments)\n",
    "                    print(f\"comments tokens: {tokens_comments}\")\n",
    "            if 'answers' in documents[i]:\n",
    "                for answer in documents[i]['answers']:\n",
    "                    tokens_answers = prepare_document_so(answer['Body'])\n",
    "                    corpus.append(tokens_answers)\n",
    "                    print(f\"answers tokens: {tokens_answers}\")\n",
    "    return corpus\n",
    "\n",
    "def build_tfidf_model_so(corpus):\n",
    "    corpus_dict = Dictionary(corpus)\n",
    "    corpus_bow = tuple(corpus_dict.doc2bow(sentence) for sentence in corpus)\n",
    "    tfidf_model = TfidfModel(corpus_bow, normalize=True)\n",
    "\n",
    "    return corpus_dict, corpus_bow, tfidf_model\n",
    "\n",
    "def get_keywords(model, num_topics=-1, num_words=5):\n",
    "    topic_vectors = model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "    return sorted(tuple(set([w[0] for t in topic_vectors for w in t[1]])))\n",
    "\n",
    "folder_path = '../step-2-output/questions'\n",
    "data = load_json_files(folder_path)\n",
    "\n",
    "# Flatten the list of documents if necessary\n",
    "documents = []\n",
    "for sublist in data:\n",
    "    if isinstance(sublist, list):\n",
    "        documents.extend(sublist)\n",
    "    else:\n",
    "        documents.append(sublist)\n",
    "\n",
    "corpus = prepare_corpus_so(documents)\n",
    "\n",
    "\n",
    "(corpus_dict, corpus_bow, tfidf_model) = build_tfidf_model_so(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd650f09-732b-456c-a9e7-c13a8da09158",
   "metadata": {},
   "source": [
    "Explore hyperparameters\n",
    "- K = {5,6,...,34,35}\n",
    "- alpha = {0.01,50/K}\n",
    "- beta = {0.01,50/K}\n",
    "- chunksize = {1,2,4,8,...,1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960353e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for num_topics in range(5, 36):\n",
    "    for alpha in (0.01, 50/num_topics):\n",
    "        for beta in (0.01, 50/num_topics):\n",
    "            for chunksize in (1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024):\n",
    "                start_time = time.time()\n",
    "                print(f\"Training LDA with num_topics={num_topics}, alpha={alpha}, beta={beta}, chunksize={chunksize}\")\n",
    "                \n",
    "                lda_model = LdaModel(\n",
    "                    corpus=corpus_bow,\n",
    "                    id2word=corpus_dict,\n",
    "                    num_topics=num_topics,\n",
    "                    alpha=alpha,\n",
    "                    eta=beta,\n",
    "                    chunksize=chunksize,\n",
    "                    passes=100\n",
    "                )\n",
    "                \n",
    "                perplexity = lda_model.log_perplexity(corpus_bow)\n",
    "                coherence_model_lda = CoherenceModel(\n",
    "                    model=lda_model,\n",
    "                    texts=corpus,\n",
    "                    dictionary=corpus_dict,\n",
    "                    coherence='c_v'\n",
    "                )\n",
    "                coherence_lda = coherence_model_lda.get_coherence()\n",
    "                top_words = lda_model.print_topics(num_words=10)\n",
    "                results.append({\n",
    "                    'num_topics': num_topics,\n",
    "                    'alpha': alpha,\n",
    "                    'beta': beta,\n",
    "                    'chunksize': chunksize,\n",
    "                    'perplexity': perplexity,\n",
    "                    'coherence': coherence_lda,\n",
    "                    'top_words': top_words\n",
    "                })\n",
    "                \n",
    "                end_time = time.time()\n",
    "                print(f\"Completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f0cba",
   "metadata": {},
   "source": [
    "### Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d6856",
   "metadata": {},
   "source": [
    "#### Display results using IPython display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e42396",
   "metadata": {},
   "source": [
    "#### Display top words for each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785568a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in results_df.iterrows():\n",
    "    print(f\"Model {index + 1}: num_topics={row['num_topics']}, alpha={row['alpha']}, beta={row['beta']}, chunksize={row['chunksize']}\")\n",
    "    for topic in row['top_words']:\n",
    "        topic_id, words_str = topic\n",
    "        words = [word.split('*')[1].strip('\"') for word in words_str.split(' + ')]\n",
    "        print(f\"Topic {topic_id}: {', '.join(words)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d96ef3",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5173d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting Coherence and Perplexity Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['num_topics'], results_df['coherence'], marker='o')\n",
    "plt.title('Coherence Score by Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_df['num_topics'], results_df['perplexity'], marker='o')\n",
    "plt.title('Perplexity Score by Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Perplexity Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to generate word cloud for each topic\n",
    "def plot_word_cloud(lda_model, num_topics):\n",
    "    for t in range(num_topics):\n",
    "        plt.figure()\n",
    "        plt.imshow(WordCloud(background_color='white').fit_words(dict(lda_model.show_topic(t, 20))))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Topic #{t}')\n",
    "        plt.show()\n",
    "\n",
    "# Generate word clouds for the best model\n",
    "best_model_index = results_df['coherence'].idxmax()\n",
    "best_model = results_df.loc[best_model_index]\n",
    "num_topics = best_model['num_topics']\n",
    "\n",
    "# Assuming lda_model is the best LDA model you want to visualize\n",
    "# Plot word clouds for each topic\n",
    "plot_word_cloud(lda_model, num_topics)\n",
    "\n",
    "# Bar plots for topics\n",
    "def plot_top_words(lda_model, num_topics, num_words=10):\n",
    "    for topic_id in range(num_topics):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        top_words = lda_model.show_topic(topic_id, num_words)\n",
    "        words, weights = zip(*top_words)\n",
    "        plt.barh(words, weights)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(f'Topic {topic_id}')\n",
    "        plt.show()\n",
    "\n",
    "# Plot top words for each topic\n",
    "plot_top_words(lda_model, num_topics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "78eb23d1a45a4e62602fd7282f0d71f9bad0427acf3dae68a8e28a953513495c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
